\documentclass{article}
\usepackage{fullpage}
\usepackage{physics}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{commath}

\newtheorem{theorem}{Theorem}

\DeclareMathOperator*{\kl}{{\rm kl}}
\DeclareMathOperator*{\h}{{\rm h}}
\DeclareMathOperator*{\KL}{{\rm KL}}
\DeclareMathOperator*{\nent}{{\rm H}}
\DeclareMathOperator*{\Bin}{{\rm B}}
\DeclareMathOperator*{\U}{{\rm U}}
\DeclareMathOperator*{\N}{{\rm N}}
\newcommand{\RR}{\mathbb{R}} % Real numbers
\newcommand{\defeq}{\triangleq}
\newcommand*{\one}{{\bf 1}}
\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}

\title{CSC 665: Calibration Homework}
\author{Chicheng Zhang}

\begin{document}
\maketitle

Please complete the following set of exercises \textbf{on your own}.
The homework is due \textbf{on Sep 3, in class}.
You may find the following version of Taylor's Theorem in multivariate
calculus helpful:
\begin{theorem}
Suppose $f$ is twice differentiable in $\RR^d$. Then given two points $a$, $b$ in
$\RR^d$, there exists some $t$ in $[0,1]$, such that
\[ f(b) = f(a) + \inner{\nabla f(a)}{b - a} + \frac 1 2 (b - a)^\top \nabla^2 f(\xi) (b-a), \]
where $\xi = t a + (1-t) b$.
Here $\nabla f (x) \defeq (\pdv{f}{x_1}, \ldots, \pdv{f}{x_d})$ is the gradient of $f$ at $x$,
and
\[ \nabla^2 f (x) \defeq \begin{bmatrix} \pdv[2]{f}{x_1} & \ldots & \pdv{f}{x_1}{x_d} \\ & \ldots &  \\ \pdv{f}{x_d}{x_1} & \ldots & \pdv[2]{f}{x_d} \end{bmatrix}\]
is the Hessian of $f$ at $x$.
\end{theorem}


\section*{Problem 1}
Denote by $\Bin(n,p)$ the binomial distribution with $n$ being the number of trials,
and $p$ being the success probability of each trial,
and denote by $\N(\mu, \sigma^2)$ the normal distribution with mean $\mu$ and variance
$\sigma^2$.

\begin{enumerate}
\item Suppose $Y$ is a random variable such that $P(Y=+1) = P(Y=-1) = \frac12$.
In addition, given $Y$, $X$ has the following conditional probability distribution:
given $Y = -1$, $X \sim \Bin(3, \frac 2 3)$; given $Y = +1$,
$X \sim \Bin(2, \frac 1 3)$. Calculate:
\begin{enumerate}
  \item the joint probability table of $(X,Y)$;
  \item $P(Y=1 | X = 3)$;
  \item $P(Y=-1|X=1)$.
\end{enumerate}

\item Suppose $Y$ is a random variable such that $P(Y=+1) = P(Y=-1) = \frac12$.
In addition, suppose given $Y$, $X$ has the following conditional probability
distribution: given $Y = -1$, $X \sim \N(\mu_-, \sigma^2)$; given $Y = +1$,
$X \sim \N(\mu_+, \sigma^2)$. Define
\[ P(Y=+1|x) \defeq \frac{P(Y=+1) p_{+1}(x)}{P(Y=+1) p_{+1}(x) + P(Y=-1) p_{-1}(x)}. \]
where $p_{+1}$ and $p_{-1}$
are the conditional probability density functions of $X$ given $Y = +1$ and
$Y = -1$ respectively.
Show that
\[ P(Y=+1|x) = \frac{1}{1+\exp(-\frac{\mu_+ - \mu_-}{\sigma^2} \cdot (x - \frac{\mu_++\mu_-}{2}) )}. \]

(Remark: $P(Y=+1|x)$ has the intuitive interpretation that it is
 the conditional probability of $Y=+1$ given $X = x$. It can be shown rigorously
 that $P(Y=+1|x) = \lim_{\epsilon \to 0} P(Y=+1|X \in [x-\epsilon, x+\epsilon])$.)
\end{enumerate}

\section*{Problem 2}
\begin{enumerate}
\item Suppose $D = \U([0,1])$, i.e. the uniform distribution over the
$[0,1]$ interval.
Consider a set of samples $S = (X_1, \ldots, X_n)$
drawn identically and independently from distribution $D$.

Write a program that plots the empirical {\em cumulative distribution
function} (CDF) of the sample $S$, that is,
\[ F_n(t) = \frac{1}{n} \sum_{i=1}^n \one(x_i \leq t), t \in \RR, \]
where
\[ \one(A) = \begin{cases} 1 & A \text{ is true } \\ 0 & A \text{ is false } \end{cases} \]
is the indicator function. You may use any programming languages you like (I recommend using Python and Jupyter
notebook).

Draw two sets of samples $S_1$ and $S_2$ of size $n = 5$. Plot $F_n^1$,
the CDF of $S_1$, and plot $F_n^2$, the CDF of $S_2$. Are they different? Why?

\label{item:2-a}

\item Repeat the same experiment in \label{item:2-a} for $n = 100$ and $n=1000$. Do
the $F_n^1$ and $F_n^2$ functions become closer as $n$ increases?
\label{item:2-b}

\item In the above experiment, as $n$ goes to infinity, what function does $F_n$ converge to?
Can you derive a formula for that function (denoted as $F$)?
\label{item:2-c}

\item Suppose $D$ is the the
standard normal distribution $\N(0,1)$, what function does $F_n$ converge to?
\end{enumerate}

\section*{Problem 3}
\begin{enumerate}
\item Define function
\[ h(x) \defeq x \ln x + (1-x) \ln (1-x), x \in (0,1). \]
Show that for any $p$ and $q$ in $(0,1)$,
\[ h(q) - h(p) - h'(p)(q - p)
   = p \ln \frac{p}{q} + (1-p) \ln \frac{1-p}{1-q}. \]
(Remark: the expression on the right hand side is often called the {\em binary relative
entropy}, denoted as $\kl(p,q)$; the $h$ function is often called the {\em negative
binary entropy}.)

\item Suppose $0 < p < q < 1$.
Use Taylor's Theorem to show that
\[ \kl(p,q) \geq 2(p-q)^2. \]
Furthermore, show that
\[ \kl(p,q) \geq \frac{(p-q)^2}{2q}. \]

\item Define the $m$-dimensional probability simplex
$\Delta^{m-1}$ as $\cbr{p \in \RR^m: \text{ for all } i, p_i \geq 0,
\sum_{i=1}^m p_i = 1}$. For two vectors $p$, $q$ in $\Delta^{m-1}$,
define the negative entropy of $p$ as:
\[ \nent(p) \defeq \sum_{i=1}^m p_i \ln p_i, \]
and the relative entropy between $p$ and $q$ as:
\[ \KL(p, q) \defeq \sum_{i=1}^m p_i \ln \frac{p_i}{q_i}. \]
Verify that
\[ \nent(q) - \nent(p) - \inner{\nabla \nent(p)}{q - p} = \KL(p, q). \]

\item Using Taylor's Theorem, show that for any $p$, $q$ in $\Delta^{m-1}$,
$\KL(p,q) \geq 0$.
Furthermore, show that
$\KL(p,q) \geq \frac{1}{2} (\sum_{i=1}^m |p_i - q_i|)^2$.

Hint: at some point, you may want to use the following variant of Cauchy-Schwarz
inequality:
\[
(\sum_{i=1}^m y_i) \cdot (\sum_{i=1}^m \frac{x_i^2}{y_i}) \geq (\sum_{i=1}^m |x_i|)^2.
\]
\end{enumerate}


\end{document}
