\documentclass{article}
\usepackage{fullpage}
\usepackage{physics}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{commath}
\usepackage{algorithm, algorithmic}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

\DeclareMathOperator*{\kl}{{\rm kl}}
\DeclareMathOperator*{\h}{{\rm h}}
\DeclareMathOperator*{\KL}{{\rm KL}}
\DeclareMathOperator*{\nent}{{\rm H}}
\DeclareMathOperator*{\Bin}{{\rm B}}
\DeclareMathOperator*{\U}{{\rm U}}
\DeclareMathOperator*{\N}{{\rm N}}
\DeclareMathOperator*{\Var}{{\rm Var}}
\DeclareMathOperator*{\err}{{\rm err}}
\DeclareMathOperator*{\Xcal}{{\cal X}}
\DeclareMathOperator*{\Hcal}{{\cal H}}
\DeclareMathOperator*{\Ycal}{{\cal Y}}
\DeclareMathOperator*{\Acal}{{\cal A}}
\DeclareMathOperator*{\argmin}{{\rm argmin}}
\newcommand{\RR}{\mathbb{R}} % Real numbers
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\defeq}{\triangleq}
\newcommand*{\one}{{\bf 1}}
\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}

\title{CSC 665: The PAC learning model}
\author{Chicheng Zhang}

\begin{document}
\maketitle

\section{The PAC learning model}

PAC stands for ``Probably Approximately Correct''~\cite{valiant1984theory}, which is a celebrated theoretical model for studying binary classification.

Setup: binary classification. Example: image classification - build a classifier
that can classify images as cats or dogs.

Basic terminologies:
\begin{enumerate}
\item Instance domain $\Xcal$ - the space where the images lie in. For example,
each image is grayscale, of resolution $640 \times 480$, and is represented by its pixel intensity.
then can take $\Xcal = \RR^{640 \times 480}$.

\item Label space $\Ycal = \cbr{-1, +1}$ - the space where labels lie in.
For example, we can use $+1$ to denote class 'cat', and $-1$ to denote class 'dog'.

\item Data distribution $D$: the distribution where examples $(x,y)$'s are drawn from.

\item Training set: a set $S$ drawn iid from $D$.

\item Classifier $h$ (hypothesis): a mapping from $\Xcal$ to $\Ycal$ - given a image as input, the
classifier will tell whether the image is a cat or a dog.

\item Hypothesis class $\Hcal$: a (structured) collection of classifiers. For example,
$\Hcal = $ the set of all neural networks with ResNet-18 architecture.
\footnote{With an additional linear threshold function at the output.}
(each different weighting combination corresponds to a different classifier in $\Hcal$)

\item Error: given a classifier $h$, define its generalization error as
$\err(h, D) = \PP_{(x,y) \sim D}(h(x) \neq y)$. This is the performance
measure of a classifier. Define its training error with respect
to the training set $S$ of size $n$ as $\err(h, S) = \frac{1}{n} \sum_{(x,y) \in S} \one(h(x) \neq y)$,
also abbreviated as $\PP_{(x,y) \sim S}(h(x) \neq y)$.

\item Realizable: there exists a classifier $h^\star$ in $\Hcal$, such that
 $\err(h^\star, D) = 0$. Agnostic: there may or may not exists a classifier in $\Hcal$
 that has zero generalization error.
\end{enumerate}

\begin{definition}
We call an algorithm $\Acal$ PAC learns hypothesis class $\Hcal$ with sample complexity
function $m: (0,1) \times (0,1) \to \NN$, if for any distribution $D$ realizable with respect to $\Hcal$, $\epsilon > 0$, $\delta > 0$,
when $\Acal$ receives $m(\epsilon, \delta)$ iid training examples from $D$ as input,
 and outputs
a classifier $h$, such that with probability $1-\delta$,
\[ \err(h, D) \leq \epsilon. \]
\end{definition}

Remarks:
\begin{enumerate}
\item $\epsilon$ is called the {\em target error rate}, and $\delta$ is called the {\em confidence parameter}~\footnote{This may remind you the notion of $(1-\alpha)$-confidence interval in statistics, where the goal is to construct an interval such that it contains the underlying parameter with probability $1-\alpha$. Here $\alpha$ is essentially $\delta$, the failure probability}. ``Probably'' means that the algorithm succeed with high probability ($1-\delta$),
and ``Approximately correct'' means that the classifier returned has small error rate ($\epsilon$).
\item The original definition in~\cite{valiant1984theory} requires that the learning algorithm runs in polynomial time. Here we relax the original definition and do not require computational efficiency.
\item The sample complexity is {\em distribution independent}. In practice, one might want a sample complexity definition that ``exploits the easiness of the data'', i.e. if distribution $D$ is easy to learn, then the learning algorithm may draw less samples from $D$ to learn a good classifier. We will return to this point when we study model selection.
%The definition does notFor a hypothesis class, there may exist multiple algorithms with different sample complexities.
\end{enumerate}

\subsection{Sample complexity of finite hypothesis classes}
Let us consider the setting where the hypothesis class $\Hcal$ is of finite size. In fact,
it can be argued that all hypothesis classes considered in computer-based applications are finite! To see this, consider for example the set of all ResNet-18 networks. As each weight can only take values in 64-bit floating-point numbers, any such network can be represented using (64 $\times$ \#weights) bits.

Here we show a fundational result: any finite hypothesis class $\Hcal$ will have a sample
complexity of order $m(\epsilon, \delta) = \frac{1}{\epsilon}(\ln|\Hcal| + \ln\frac{1}{\delta})$.
To see this, let us consider the following simple learning algorithm.

\begin{algorithm}
\caption{The consistency algorithm}
\begin{algorithmic}[1]
  \REQUIRE{Training samples $S$ of size $m$ iid from $D$, hypothesis class $\Hcal$}
  \STATE{Find $\hat{h}$ such that $\hat{h}$ agrees with all examples in $S$, that is,
  for all $(x,y)$ in $S$, $\hat{h}(x) = y$.}
  \label{step:cons}
  \RETURN{$\hat{h}$.}
\end{algorithmic}
\label{alg:cons}
\end{algorithm}

\begin{theorem}
  Suppose $\Hcal$ is finite. If the consistency algorithm (Algorithm~\ref{alg:cons}) is given $m$ iid
  examples from $D$ realizable with respect to $\Hcal$, then with probability $1-\delta$,
  its output $\hat{h}$ is such that
  \begin{equation}
    \err(\hat{h}, D) \leq \epsilon_R(m,\delta,\ln|\Hcal|) \defeq \frac{\ln|\Hcal| + \ln\frac{1}{\delta}}{m}.
    \label{eqn:eb}
  \end{equation}
  In other words, it PAC learns hypothesis class $\Hcal$ with sample complexity
  $m(\epsilon, \delta) = \frac{1}{\epsilon}(\ln|\Hcal| + \ln\frac{1}{\delta})$.
  \label{thm:sc}
\end{theorem}

We make the following obervations on the error upper bound (Equation~\eqref{eqn:eb}). First, the dependence on sample size
is of order $\frac{1}{m}$. This is reasonable, as it is polynomially decreasing. Second, the dependency on the size of the hypothesis class is only logarithmic. This is somewhat desirable, as $|\Hcal|$ is usually exponential in the number
of parameters in $\Hcal$, and taking a logarithm will bring the dependency on the number of parameters from exponential to linear \footnote{In modern applications such as learning overparameterized neural networks, this bound is not desirable - the number of parameters can be much greater than the training sample size, which makes this bound vacuous ($\geq 1$).}. Third, the dependency on $\delta$ is only $\ln\frac{1}{\delta}$ - this means that the failure probability $\delta$ can be taken as a small number (such as $m^{-5}$) without hurting much on error bound.

\begin{proof}[Proof of Theorem~\ref{thm:sc}]

  First, under realizable assumption, $h^\star$ has zero error with respect to $D$. Therefore, (with probability 1) $h^\star$ will also agree with all examples in $S$.
  This implies that Step~\ref{step:cons} will successfully find a classifier $\hat{h}$ that agrees with all exmaples in $S$.

  Second, let $\mu:= \epsilon_R(m,\delta,|\Hcal|)$. Let us consider all classifiers in $\Hcal$ that has error $> \mu$ - define such subset as $\Hcal_\mu$. We argue that with probability $1-\delta$, all members of $\Hcal_\mu$ has nonzero error in $S$.

  Fix $h$ in $\Hcal_\mu$. Denote by the event $E_h$ that $h$ has zero error in $S$. Then
  \[ \PP(E_h) = \PP(h(x) = y)^m = (1 - \err(h, D))^m < (1 - \mu)^m
  \leq e^{-\mu m}. \]

  Define $E := \cup_{h \in \Hcal_\mu} E_h$.
  By union bound over all $h$ in $\Hcal_\mu$, we have
  \[ \PP(E) \leq \sum_{h \in \Hcal_\mu} \PP(E_h) \leq |\mathcal{H}_{\mu}| e^{-\mu m} \leq |\Hcal| e^{-\mu m}. \]

  Plugging in the value of $\mu = \frac{\ln|\Hcal| + \ln\frac{1}{\delta}}{m}$, we get that $\PP(E)$ is at most the right hand side, which is at most $\delta$.

  Event $E$ states that there exists $h$ in $\Hcal_\mu$ that agrees with $S$. Its complement $\bar{E}$ states that for all $h$ in $\Hcal_\mu$, $h$ does not agrees with $S$.

  Suppose event $\bar{E}$ happens. In this case, as the output $\hat{h}$ agrees with $S$, $\hat{h}$ must be outside of $\Hcal_\mu$, that is, it has error at most $\mu$. The first part of the theorem is concluded by observing that $\bar{E}$ happens with probability at least $1-\delta$.

  The sample complexity bound directly follows by plugging into the choice of $m$ into
  the excess error bound $\epsilon_R$, which becomes $\epsilon$.

\end{proof}

\section{The Agnostic PAC learning model}

\begin{definition}
We call an algorithm $\Acal$ agnostic PAC learns hypothesis class $\Hcal$ with sample complexity
function $m: (0,1) \times (0,1) \to \NN$, if for any distribution $D$, $\epsilon > 0$, $\delta > 0$,
when $\Acal$ receives $m(\epsilon, \delta)$ iid training examples from $D$ as input,
 and outputs
a classifier $h$, such that with probability $1-\delta$,
\[ \err(h, D) \leq \min_{h' \in \Hcal} \err(h',D) + \epsilon. \]
\end{definition}

\subsection{Agnostic sample complexity of finite hypothesis classes}
\begin{algorithm}
\caption{Empirical risk minimzation}
\begin{algorithmic}[1]
  \REQUIRE{Training samples $S$ of size $m$ iid from $D$, hypothesis class $\Hcal$}
  \STATE{Find $\hat{h}$ such that $\hat{h}$ has the lowest training error among $\Hcal$:
  $\hat{h} \in \argmin_{h \in \Hcal} \err(h,S)$.}
  \label{step:cons}
  \RETURN{$\hat{h}$.}
\end{algorithmic}
\label{alg:erm}
\end{algorithm}

\begin{theorem}
  Suppose $\Hcal$ is finite. If the consistency algorithm (Algorithm~\ref{alg:erm}) is given $m$ iid
  examples from $D$ realizable with respect to $\Hcal$, then with probability $1-\delta$,
  its output $\hat{h}$ is such that
  \begin{equation}
    \err(\hat{h}, D) \leq \min_{h \in \Hcal} \err(h, D) + \epsilon_A(m,\delta,\ln|\Hcal|) \defeq \sqrt{\frac{2(\ln|\Hcal| + \ln\frac{2}{\delta})}{m}}.
    \label{eqn:eb-a}
  \end{equation}
  In other words, it agnostic PAC learns hypothesis class $\Hcal$ with sample complexity
  $m(\epsilon, \delta) = \frac{2}{\epsilon^2}(\ln|\Hcal| + \ln\frac{2}{\delta})$.
  \label{thm:sc-a}
\end{theorem}

\begin{proof}
Let $\mu = \frac{1}{2}\epsilon_A(m,\delta,\ln|\Hcal|) = \sqrt{\frac{\ln|\Hcal| + \ln\frac{2}{\delta}}{2m}}$.

For all classifiers $h$ in $\Hcal$, define $E_h$ as the event that
\[ |\err(h, S) - \err(h, D)| > \mu. \]
In addition, define $E = \cup_h \in \Hcal E_h$.

By Hoeffding's Inequality,
\[ \PP(E_h) \leq 2 e^{-2 m \mu^2}. \]

Therefore, by union bound and expanding the definition of $\mu$, we get that
\[ \PP(E) \leq \sum_{h \in \Hcal} \PP(E_h) \leq |\Hcal| \cdot 2 e^{-2 m \mu^2} \leq \delta. \]

For the rest of the proof, suppose event $\bar{E}$ happens. We note that $\bar{E}$ happens with probability at least $1-\delta$.

we have that for all $h$,
\[ |\err(h, S) - \err(h, D)| \leq \mu \]
Specifically,
\begin{eqnarray*}
  \err(\hat{h}, D)
  &\leq& \err(\hat{h}, S) + \mu \\
  &=& \min_{h \in \Hcal} err(h, S) + \mu \\
  &\leq& \min_{h \in \Hcal} err(h, D) + 2 \mu \\
  &\leq& \min_{h \in \Hcal} err(h, D) + \epsilon_A(m,\delta,\ln|\Hcal|).
\end{eqnarray*}

The sample complexity bound directly follows by plugging into the choice of $m$ into
the excess error bound $\epsilon_A$, which becomes $\epsilon$.

\end{proof}


\bibliographystyle{plain}
\bibliography{learning}

\end{document}
