\documentclass{article}
\usepackage{fullpage}
\usepackage{physics}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{commath}
\usepackage{algorithm, algorithmic}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

\DeclareMathOperator*{\kl}{{\rm kl}}
\DeclareMathOperator*{\h}{{\rm h}}
\DeclareMathOperator*{\KL}{{\rm KL}}
\DeclareMathOperator*{\nent}{{\rm H}}
\DeclareMathOperator*{\Bin}{{\rm B}}
\DeclareMathOperator*{\U}{{\rm U}}
\DeclareMathOperator*{\N}{{\rm N}}
\DeclareMathOperator*{\Var}{{\rm Var}}
\DeclareMathOperator*{\err}{{\rm err}}
\DeclareMathOperator*{\Xcal}{{\cal X}}
\DeclareMathOperator*{\Hcal}{{\cal H}}
\DeclareMathOperator*{\Ycal}{{\cal Y}}
\DeclareMathOperator*{\Acal}{{\cal A}}
\DeclareMathOperator*{\Zcal}{{\cal Z}}
\DeclareMathOperator*{\Fcal}{{\cal F}}
\DeclareMathOperator*{\Scal}{{\cal S}}
\DeclareMathOperator*{\argmin}{{\rm argmin}}
\DeclareMathOperator*{\VC}{{\rm VC}}
\newcommand{\RR}{\mathbb{R}} % Real numbers
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\defeq}{\triangleq}
\newcommand*{\one}{{\bf 1}}
\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}

\title{CSC 665: Vapnik-Chervonenkis (VC) Theory}
\author{Chicheng Zhang}

\begin{document}
\maketitle

\section{Infinite hypothesis classes can be PAC learnable}
In the last lecture, we have seen that the size of a hypothesis class $\Hcal$
can be a important factor of sample complexity of learning from that $\Hcal$.
Specifically, if $\Hcal$ is finite, then it has a PAC sample complexity upper bound
 of $O(\frac{1}{\epsilon}(\ln|\Hcal| + \ln\frac1\delta))$, and an agnostic PAC
sample complexity upper bound of $O(\frac{1}{\epsilon^2}(\ln|\Hcal| + \ln\frac1\delta))$.
Does that mean that if $\Hcal$ is infinite, then $\Hcal$ is not PAC learnable?

In this section, we give a counterexample, showing that for the hypothesis
class of threshold functions on the $[0,1]$ interval, $\Hcal$ is PAC learnable.
To formalize the statement, we need some notation setup.
\begin{enumerate}
  \item The instance domain $\Xcal$ be the $[0,1]$ interval,
  \item The label space $\Ycal$ be $\cbr{-1,+1}$.
  \item The hypothesis class $\Hcal = \cbr{h_t \defeq 2 \one(x > t)-1:  t \in [0,1]}$
is the set of threshold functions over $[0,1]$. Given classifier $h_t$, it will classify
all examples $x$ on the left of $t$ as label $-1$, and classify all examples $x$ on the right
of $t$ (including $t$) as label $+1$. Note that $\Hcal$ is (uncountably) infinite.
\end{enumerate}

Recall that the consistency algorithm is one that returns a classifier $\hat{h}$ in $\Hcal$
that agrees with all training examples.
We have the following theorem on the sample complexity of the consistency algorithm.

\begin{theorem}
Suppose $D$ is a distribution over $[0,1]$ that is realizable with respect to $\Hcal$.
Then, for any $\epsilon \in (0,\frac12)$, $\delta \in (0,1)$, given $m \geq \frac{1}{\epsilon} \ln\frac{2}{\delta}$ training examples drawn iid from $D$,
the consistency algorithm returns a classifier $h_{\hat{t}}$ such that with probability $1-\delta$,
\[ \err(h_{\hat{t}}, D) \leq \epsilon. \]
\label{thm:pac-thres}
\end{theorem}

\begin{proof}
We will only consider the setting where $D$ is a continuous probability distribution that has density on $[0,1]$. (For a rigorous proof for general $D$, see Appendix~\ref{app:rig} for details.)

Consider two points $t_L$ and $t_R$, which are defined such that
\[ \PP(x \in [t^\star, t_R]) = \epsilon, \]
\[ \PP(x \in [t_L, t^\star]) = \epsilon. \]
We consider the setting where such $t_L$ and $t_R$ exists. See Appendix~\ref{app:rig} for the proof for the general setting where $t_L$ and $t_R$ may not exist.

We are going to show that given a sample size $m \geq \frac{1}{\epsilon} \ln\frac{2}{\delta}$, there exists an event $\bar{E}$, such that at least one sample is in $[t^\star, t_R]$, and at least one sample is in $[t_L, t^\star]$. Note that if this happens, then the returned threshold $\hat{t}$ will be inside $[t_L, t_R]$.

If $\hat{t}$ is in $[t_L, t^\star]$, then
\[ \err(h_{\hat{t}}, D) = \PP(x \in [\hat{t}, t^\star] \leq \PP(x \in [t_L, t^\star]) = \epsilon. \]
Similarly, if $\hat{t}$ is in $[t^\star, t_R]$, then
\[ \err(h_{\hat{t}}, D) = \PP(x \in [t^\star, \hat{t}] \leq \PP(x \in [t^\star, t_R]) = \epsilon. \]

Now define event $E_L$ (resp. $E_R$) be such that no sample is in $[t_L, t^\star]$ (resp. $[t^\star, t_R]$), and define $E = E_L \cup E_R$. It suffices to show $\PP(E) \leq \delta$.
Indeed,
\[ \PP(E_L) = (1 - \PP(x \in [t_L, t^\star]))^m \leq e^{-\epsilon m} = \delta/2\]
and similarly $\PP(E_R) \leq \delta/2$. This implies that $\PP(E) \leq \PP(E_L) + \PP(E_R) \leq \delta$.
\end{proof}

\section{VC dimension}
We provide a more refined characterization of the complexity of a hypothesis class. Generally, if a hypothesis class is more expressive, then we may need more samples
to learn from them. But how can we measure the expressiveness of a hypothesis class?

\begin{definition}
Given a hypothesis class $\Hcal$ and a set of unlabeled examples $S = \cbr{x_1, \ldots, x_n}$, define the projection of $\Hcal$ to $S$ as:
\[ \Pi_{\Hcal}(S) = \cbr{(h(x_1), \ldots, h(x_n)): h \in \Hcal}. \]
\end{definition}

Intuitively, if $\Hcal$ is more expressive, then $|\Pi_{\Hcal}(S)|$ is larger. The largest possible value of $|\Pi_{\Hcal}(S)|$ is $2^n$, where $\Hcal$ achieves all possible $+1/-1$ labelings on $S$. In this case, we call that $S$ is {\em shattered} by $\Hcal$.

\begin{definition}
The VC dimension of $\Hcal$ (abbrev. $\VC(\Hcal)$), is the largest nonnegative integer $d$ such that there exists $S$ of size $d$ that is shattered by $\Hcal$. If no such $d$ exists,
we $\VC(\Hcal)$ is defined to be infinity.
\end{definition}

We have the following more checkable definition of VC dimension:
\begin{lemma}
Suppose we are given a hypothesis class $\Hcal$ and an integer $d$.
Then $\VC(\Hcal) = d$ is equivalent to the following two holding simultaneously:
\begin{enumerate}
\item There exists a set of examples of size $d$ that is shattered by $\Hcal$.
\item Any set of examples of size $d+1$ are not shattered by $\Hcal$.
\end{enumerate}
\end{lemma}


Examples of VC dimension:
\begin{enumerate}
\item Thresholds in $\RR$. $\Hcal = \cbr{h_t(x) \defeq 2 \one(x > t)-1:  t \in [0,1]}$. It can be seen that $\cbr{0.5}$ is shattered by $\Hcal$. However, consider any set $S = \cbr{x_1, x_2}$. Suppose $x_1 \leq x_2$. Then it is impossible to find $h_t$ in $\Hcal$ such that
$h_t(x_1) = +1$ and $h_t(x_2) = -1$. Therefore, $\VC(\Hcal) = 1$.

\item Intervals in $\RR$. $\Hcal = \cbr{h_{a,b}(x) \defeq 2\one(a \leq x \leq b) - 1:  t \in [0,1]}$. It can be seen that $\cbr{0.2, 0.5}$ is shattered by $\Hcal$. However, consider any set $S = \cbr{x_1, x_2, x_3}$. Suppose $x_1 \leq x_2 \leq x_3$. Then it is impossible to find $h_{a,b}$ in $\Hcal$ such that
$h_{a,b}(x_1) = +1$, $h_{a,b}(x_2) = -1$, $h_{a,b}(x_3) = +1$. The reason is that: $h_{a,b}(x_1) = +1$ implies that $a \leq x_1$; $h_{a,b}(x_3) = +1$ implies that $x_3 \leq b$. However, this would imply that $x_2 \in [a,b]$, therefore $h_{a,b}(x_2) = +1$,
and $h_{a,b}(x_2) = -1$ is impossible.
Therefore, $\VC(\Hcal) = 2$.

\item Homogeneous linear classifiers in $\RR^d$. $\Hcal = \cbr{h_w(x) \defeq 2\one(w \cdot x > 0) - 1: w \in \RR^d}$. It can be seen that the canonical basis vectors $\cbr{e_1, \ldots, e_d}$ (or more generally, any set of linearly independent examples) is shattered by $\Hcal$. To see this, note that given a set of linearly independent examples $x_1, \ldots, x_m$, consider the matrix
$M \in \RR^{m \times d}$ whose rows are the $x_i$'s. Note that $M$ has rank $m$, therefore its columns also spans the whole $\RR^m$. Hence, for any vector $l$ in $\RR^m$, there is a vector $w$ in
$\RR^d$, such that
\[ M w = \begin{bmatrix} \inner{w}{x_1} \\ \ldots \\ \inner{w}{x_d} \end{bmatrix} = l. \]
This immediately implies that for any labeling in $\cbr{-1,+1}^m$, there is a linear classifier in $\RR^m$ that achieves that labeling. Hence, $\VC(\Hcal) \geq d$.

However, consider any set $S = \cbr{x_1, \ldots, x_{d+1}}$. We now show that $S$ is not shatterable.

First, $x_1, \ldots, x_{d+1}$ are $d+1$ vectors in $\RR^d$, therefore they must be linearly dependent. Thus, there exists $\alpha_1, \ldots, \alpha_{d+1}$ not all zero, such that
\begin{equation}
  \sum_{i=1}^{d+1} \alpha_i x_i = 0.
  \label{eqn:lc}
\end{equation}

Furthermore, there exists $\alpha_1, \ldots, \alpha_{d+1}$, such that there exists $i^\star$ in $\cbr{1,\ldots,d+1}$, $\alpha_i^\star > 0$,
\[ \sum_{i=1}^{d+1} \alpha_i x_i = 0. \]
The reason is as follows: if there already exists a positive $\alpha_i$ in Equation~\ref{eqn:lc}, then we are done; otherwise, we can flip the sign of the $\alpha_i$'s and ensuring at least one positive $\alpha_i$.

Now consider the following labeling $(l_1, \ldots, l_{d+1})$, where $l_i = \begin{cases} +1, \alpha_i > 0 \\ -1, \alpha_i \leq 0 \end{cases}$. Can a linear classifier achieve such labeling? Suppose there is a $w$ that achieves so. Then, for all $i$ in $\cbr{1,\ldots,d+1}$,
\[
\begin{cases} w \cdot x_i > 0, \alpha_i > 0 \\ w \cdot x_i \leq 0, \alpha_i \leq 0 \end{cases}
\]

Thus, for all $i$, $\alpha_i \inner{w}{x_i} \geq 0$. Specifically, for index $i^\star$, $\alpha_{i^\star} \inner{w}{x_{i^\star}} > 0$.
Summing over all $i$'s, this implies that
\[
  \sum_{i=1}^{d+1} \alpha_i \inner{w}{x_i} > 0.
\]
This contradicts Equation~\ref{eqn:lc}, which would imply that
\[
  \sum_{i=1}^{d+1} \alpha_i \inner{w}{x_i}= 0.
\]

\item Non-homogeneous linear classifiers in $\RR^d$.
$\Hcal = \cbr{h_w(x) \defeq 2\one(w \cdot x + w_0 > 0) - 1: (w, w_0) \in \RR^{d+1}}$.
Using the same reasoning as above, it can be shown that the VC dimension of $\Hcal$ is
$d+1$. We leave the proof to you as an exercise.
\end{enumerate}

Finally, we define the notion of growth function, which measures the largest possible number of for $\Hcal$ to datasets of fixed size $n$.
\begin{definition}
Define the growth function $\Scal(\Hcal, n)$ as the maximum number of labelings one can generate on a dataset of size $n$, formally,
\[ \Scal(\Hcal, n) = \max_{S: |S| = n} |\Pi_{\Hcal}(S)|. \]
\end{definition}

We also have the simple observation for finite classes.
\begin{lemma}
If $\Hcal$ is finite, then $\Scal(\Hcal, n) \leq |\Hcal|$ and $\VC(\Hcal) \leq \log_2 |\Hcal|$.
\end{lemma}
\begin{proof}
The first statement is trivial as $|\Pi_{\Hcal}(S)| \leq |\Hcal|$. For the second statement, suppose $\Hcal$ shatters $S$.
Then, $2^{|S|} \leq |\Pi_{\Hcal}(S)| \leq |\Hcal|$, implying that $|S| \leq \log_2 |\Hcal|$.
Therefore, $\VC(\Hcal)$, the maximum sizes of a dataset shatterable by $\Hcal$ is at most $\log_2 |\Hcal|$.
\end{proof}

\section{Sauer's Lemma: bounding the growth function}
Suppose we have a hypothesis class $\Hcal$ of VC dimension $d$, and a set of $m$
examples $\cbr{x_1, \ldots, x_m}$. We already know that when $m \leq d$,
$\Scal(\Hcal, n)$ can be as large as $2^m$. Can we give a good characterization of $\Scal(\Hcal, n)$ when $m > d$ (other than the trivial upper bound of $2^m-1$)? We have the following important combinatorial lemma, discovered independently by several authors (including Sauer, Shelah, Perles, Vapnik and Chervonenkis) in the 70s.

\begin{theorem}[Sauer's Lemma]
Suppose $\Hcal$ is a nonempty hypothesis class, and $S = \cbr{x_1,\ldots,x_n}$ is a set of $m$ unlabeled examples. Then,
\[ |\Pi_{\Hcal}(S)| \leq |\cbr{T \subseteq S: \Hcal \text{ shatters } T}|. \]
Consequently, if $\VC(\Hcal) = d$, then
\[ \Scal(\Hcal, m) \leq \sum_{i=0}^d {m \choose i}. \]
(The right hand side is often abbreviated as $m \choose \leq d$.) Here we use the convention that $\Hcal$ always shatters an empty set.
\end{theorem}

\paragraph{Remark.} We see that the growth function, as a function of $m$, has the following behavior on its upper bound: when $m \leq d$, the upper bound grows exponentially with $m$; however, when $m > d$, the upper bound grows as a polynomial of $m$, which is substantially slower. We will see in the next section why this type of growth is useful for establishing uniform convergence guarantees.

\begin{proof}
We will show the first claim by induction on the size of sample $m$.

\begin{itemize}
\item Base case. If $m = 1$, then there are two subcases to consider: if $\Hcal$ classifies $x_1$ in both $+1$ and $-1$ labels, then the left hand size is 2, and the right hand side is also 2. Otherwise, $\Hcal$ classifies $x_1$ in only one label, then both sides are equal to 1.

\item Inductive case. Before proceeding, we need the following important definition. Define a modification of the original hypothesis class $\Hcal$: for every labeling $(l_1, \ldots, l_m)$ in $\Pi_{\Hcal}(S)$, we select one representative classifier $h$ in $\Hcal$ that achieves the labeling; we call the collection of the classifers selected $\Hcal_S$. Note that $|\Hcal_S| = \Pi_{\Hcal}(S)$. In addition, define $S' = \cbr{x_1,\ldots,x_{m-1}}$.

Now, given $\Hcal_S$, let us decompose it to two hypothesis classes, $\Hcal_1$ and  $\Hcal_2$, in the following manner. Consider a labeling $(l_1, \ldots, l_{m-1})$ achieved by $\Hcal_2$ on examples $S'$.
\begin{itemize}
\item If both $(l_1, \ldots, l_{m-1}, +1)$ and $(l_1, \ldots, l_{m-1}, -1)$ are achievable by $\Hcal_S$, then we allocate the pair of classifiers such that one of them goes to $\Hcal_1$, and the other goes to $\Hcal_2$.
\item Otherwise, only one of $(l_1, \ldots, l_{m-1}, +1)$ and $(l_1, \ldots, l_{m-1}, -1)$ is achievable by $\Hcal_S$, then we send the classifier that achieves that labeling to $\Hcal_1$.
\end{itemize}
See Tables~\ref{tab:h_s},~\ref{tab:h_1} and~\ref{tab:h_2} for an example.

\begin{table}[H]
\centering
\begin{tabular}{l|llll}
Classifier & $x_1$ & $x_2$ & $x_3$ & $x_4$ \\
\hline
$h_1$ & $-$ & $-$ & $-$ & $-$ \\
$h_2$ & $-$ & $-$ & $-$ & $+$ \\
$h_3$ & $-$ & $+$ & $-$ & $+$ \\
$h_4$ & $+$ & $-$ & $-$ & $-$ \\
$h_5$ & $+$ & $-$ & $-$ & $+$ \\
\end{tabular}
\caption{An example with $m = 4$ and $|\Hcal_S| = 5$. The matrix shows $\Hcal_S$'s labelings on
$\cbr{x_1,x_2,x_3,x_4}$.}
\label{tab:h_s}
\end{table}

\begin{minipage}{0.45\linewidth}
\begin{table}[H]
\centering
\begin{tabular}{l|llll}
Classifier & $x_1$ & $x_2$ & $x_3$ & $x_4$ \\
\hline
$h_1$ & $-$ & $-$ & $-$ & $-$ \\
$h_3$ & $-$ & $+$ & $-$ & $+$ \\
$h_5$ & $+$ & $-$ & $-$ & $+$ \\
\end{tabular}
\caption{$\Hcal_2$'s labelings on
$\cbr{x_1,x_2,x_3,x_4}$.}
\label{tab:h_1}
\end{table}
\end{minipage}
\begin{minipage}{0.45\linewidth}
\begin{table}[H]
\centering
\begin{tabular}{l|llll}
Classifier & $x_1$ & $x_2$ & $x_3$ & $x_4$ \\
\hline
$h_2$ & $-$ & $-$ & $-$ & $+$ \\
$h_4$ & $+$ & $-$ & $-$ & $-$ \\
\end{tabular}
\vspace{0.45cm}
\caption{$\Hcal_1$'s labelings on
$\cbr{x_1,x_2,x_3,x_4}$.}
\label{tab:h_2}
\end{table}
\end{minipage}

By construction, we have the following three simple but important observations:
\begin{claim}
\begin{enumerate}
\item $|\mathcal{H}_1| = |\Pi_{\Hcal_1}(S')|$, $|\mathcal{H}_2| = |\Pi_{\Hcal_2}(S')|$.
\item If $T \subset S'$ and $\mathcal{H}_1$ shatters $T$, then it is also achieved by $\Hcal_S$.
\item If $T \subset S'$ and $\mathcal{H}_2$ shatters $T$, then $\mathcal{H}_S$ shatters $T \cup \cbr{x_m}$.
\label{claim:obs}
\end{enumerate}
\end{claim}

Now, let us upper bound the size of $\mathcal{H}_S$:
\begin{eqnarray*}
|\mathcal{H}_S| &=& |\mathcal{H}_1| + |\mathcal{H}_2| \\
&=& |\Pi_{\Hcal_1}(S')| + |\Pi_{\Hcal_2}(S')| \\
&\leq& |\cbr{T \subseteq S': T \text{ shattered by } \mathcal{H}_1}| + |\cbr{T \subset S': T \text{ shattered by } \mathcal{H}_2}| \\
&\leq& |\cbr{T \subseteq S': T \text{ shattered by } \mathcal{H}}| + |\cbr{T \subseteq S': T \cup {x_m} \text{ shattered by } \mathcal{H}}| \\
&=& |\cbr{T \subseteq S: x_m \notin T, T \text{ shattered by } \mathcal{H}}| + |\cbr{T \subseteq S: x_m \in T, T \text{ shattered by } \mathcal{H}}| \\
&=& |\cbr{T \subseteq S: T \text{ shattered by } \mathcal{H}}|
\end{eqnarray*}

For the second statement, observe that all subsets $T$ shatterable by $\Hcal$ is of size at most $d$. The right hand size of exactly the number of subsets of size at most $d$. \qedhere
\end{itemize}
\end{proof}

\begin{proof}[Proof of Claim~\ref{claim:obs}]
We show the three items respectively.
\begin{enumerate}
\item The first statement is trivial, as by construction, for every labeling $(l_1,\ldots,l_{m-1})$, there is at most one classifier in $\Hcal_1$ (resp. $\Hcal_2$).

\item The second statement is also trivial, as $\Hcal_1$ is a subset of $\Hcal_S$.

\item Suppose some classifier $h$ in $\Hcal_2$ achieves certain labeling $(b_1, \ldots, b_{|T|})$ on $T$.
Suppose $h$'s full labeling on $S'$ is $(l_1, \ldots, l_{m-1})$ (which is consistent with $(b_1, \ldots, b_{|T|})$). Then by construction, both $(l_1, \ldots, l_{m-1},+1)$ and $(l_1, \ldots, l_{m-1},-1)$ are achieved by $\Hcal_S$. This implies that $\Hcal_S$ achieves labelings
$(b_1, \ldots, b_{|T|},+1)$ and $(b_1, \ldots, b_{|T|},+1)$ on $T \cup \cbr{x_m}$. Therefore, if $\Hcal_2$ achieves all $2^{|T|}$ labelings on $T$, then $\Hcal_S$ achieves all $2^{|T|+1}$ labelings on $T \cup \cbr{x_m}$.

\paragraph{Example.} Consider the example in Tables~\ref{tab:h_s} and~\ref{tab:h_2}. Observe that $\Hcal_2$ shatters $T = \cbr{x_1}$ with classifiers $h_2$ and $h_4$.
It can be seen that $\Hcal_S$ also shatters $T \cup \cbr{x_4} = \cbr{x_1, x_4}$ with classifiers $h_1, h_2, h_4$ and $h_5$.)
\qedhere
\end{enumerate}
\end{proof}

\paragraph{Remark.} The growth function bound ${m \choose {\leq d}}$ can further be upper bounded by $m^{d+1}$ or $(\frac{em}{d})^d$.

\appendix

\section{A rigorous proof of Theorem~\ref{thm:pac-thres}}
\label{app:rig}

\begin{proof}
  As $D$ is realizable wrt $\Hcal$, there exists a classifier $h_{t^\star}$ that has zero error on $D$.

  Let us consider two critical thresholds $t_L$ and $t_R$, defined as follows:
  \[ t_L = \sup\cbr{t \in [0,1]: \PP(t \leq x \leq t^\star) \geq \epsilon} \]
  If $\PP(0 \leq x \leq t^\star) < \epsilon$, then $t_L$ is defined as $0$.

  \[ t_R = \inf\cbr{t \in [0,1]: \PP(t^\star < x \leq t) \geq \epsilon} \]
  If $\PP(t^\star < x \leq 1) < \epsilon$, then $t_R$ is defined as $1$.

  Suppose for the moment that both $t_L$ and $t_R$ are in $(0,1)$.
  Our plan is to show the following:
  \begin{enumerate}
    \item With probability $1-\delta$, the returned threshold $\hat{t}$ lies in $[t_L, t_R)$.
    \item Wherever $\hat{t}$ lies in $[t_L, t_R)$, $h_{\hat{t}}$ has error at most $\epsilon$.
  \end{enumerate}

  We show the two items respectively:
  \begin{enumerate}
    \item By Lemma~\ref{lem:discont}, we have that
    \[ \PP(t^\star < x \leq t_R) \geq \epsilon, \quad \PP(t_L \leq x \leq t^\star) \geq \epsilon. \]

    Now, consider event $E_L$ (resp. $E_R$) as the one that for all $i$, none of $x_i$ are in $[t_L, t^\star]$ (resp. $(t^\star, t_R]$). In addition, define $E = E_L \cup E_R$.

    Observe that
    \[ \PP(E_L) = \PP(\text{for all } i, x_i \notin [t_L, t^\star]) \leq (1-\epsilon)^m \leq e^{-m\epsilon} \leq \delta/2. \]
    Similarly, $\PP(E_R) \leq \delta/2$. By union bound, $\PP(E) \leq \PP(E_L) + \PP(E_R) \leq \delta$. Therefore, in the event $\bar{E}$ (which happens with probability $1-\delta$), there is an $x_i$ (resp. $x_j$) in $[t_L, t^\star]$ (resp. $(t^\star, t_R]$).
    Note that $x_i$ has label $-1$ and $x_j$ has label $+1$. Thus, the consistency algorithm
    will return a threshold $\hat{t}$ between $[t_L, t_R)$ (Note that $\hat{t}$ cannot be $t_R$, as this would misclassify $x_j$).


    \item Suppose $\bar{E}$ happens.
    We show that the generalization error of the returned threshold classifier $h_{\hat{t}}$ is at most $\epsilon$.
    \begin{enumerate}
      \item Suppose $\hat{t} < t^\star$. As argued above, $\hat{t} \geq t_L$. Therefore,
      \[ \err(h_{\hat{t}}, D) = \PP(\hat{t} < x \leq t^\star) \leq \PP(t_L < x \leq t^\star) \leq \epsilon, \]
      where the inequality is from item 1 of Lemma~\ref{lem:discont}.

      \item Suppose $\hat{t} \geq t^\star$. As argued above, $\hat{t} < t_R$. Therefore,
      \[ \err(h_{\hat{t}}, D) = \PP(t^\star < x \leq \hat{t}) \leq \PP(t^\star < x < t_R) \leq \epsilon, \]
      where the inequality is from item 2 of Lemma~\ref{lem:discont}.
    \end{enumerate}
  \end{enumerate}

  %In addition, by scrutinizing the above proof, we in fact have shown a slightly stronger statement: on event $\bar{E}$, if $t_L > 0$ and $\hat{t}$ in $[t_L, t^\star]$ then

  Now for the general case, where $t_L$ can be $0$ or $t_R$ can be $1$. Note that both cannot happen at the same time.
  Suppose $t_L = 0$, then by the exact same reasoning, we can show that with probability $1-\delta$, $\hat{t}$ is in $[t^\star, t_R)$ or $[0,t^\star]$. In the former case, as have been argued before,
  \[ \err(h_{\hat{t}}, D) \leq \PP(t^\star \leq x \leq \hat{t}) \leq \PP(t^\star \leq x < t_R) \leq \epsilon. \]

  In the latter case,
  \[ \err(h_{\hat{t}}, D) = \PP(\hat{t} < x \leq t^\star) \leq \PP(0 \leq x \leq t^\star) \leq \epsilon. \]

  In summary, with probability $1-\delta$, $\err(h_{\hat{t}}, D) \leq \epsilon$. The case of $t_R = 1$ is symmetric and is left as exercise.
\end{proof}

The following lemma crucially uses the continuity property of probability measure, that is,
If $A_1 \subset \ldots \subset A_n \subset \ldots$, and $A = \cup_{n=1}^{\infty} A_n$ (abbrev. $A_n \uparrow A$), then $\lim_{n \to \infty} \PP(A_n) = \PP(A)$.

\begin{lemma}
\begin{enumerate}
\item Suppose $\PP(0 \leq x \leq t^\star) \geq \epsilon$. Consider
\[ t_L \defeq \sup\cbr{t \in [0,1]: \PP(t \leq x \leq t^\star) \geq \epsilon}. \]
Then,
\[ \PP(t_L \leq x \leq t^\star) \geq \epsilon, \]
\[ \PP(t_L < x \leq t^\star) \leq \epsilon. \]

\item Suppose $\PP(t^\star < x \leq 1) \geq \epsilon$.
Consider
\[ t_R \defeq \inf\cbr{t \in [0,1]: \PP(t^\star < x \leq t) \geq \epsilon}. \]
Then,
\[ \PP(t^\star < x \leq t_R) \geq \epsilon, \]
\[ \PP(t^\star < x < t_R) \leq \epsilon. \]
\end{enumerate}
\label{lem:discont}
\end{lemma}
\begin{proof}
We only show the first item. The second item is left as an exercise.

First, by the definition of $t_L$, for all $t < t_L$,
$\PP(t \leq x \leq t^\star) \geq \epsilon$. As events
$\cbr{t_L - \frac{1}{n} \leq x \leq t^\star} \downarrow \cbr{t_L \leq x \leq t^\star}$ as $n \to \infty$,
this implies that
\[ \PP(t_L \leq x \leq t^\star) = \lim_{n \to \infty} \PP(t_L - \frac{1}{n} \leq x \leq t^\star) \geq \epsilon. \]

Second, by the definition of $t_L$, for all $t > t_L$,
$\PP(t \leq x \leq t^\star) < \epsilon$. As events
$\cbr{t_L + \frac{1}{n} \leq x \leq t^\star} \downarrow \cbr{t_L < x \leq t^\star}$ as $n \to \infty$,
this implies that
\[ \PP(t_L < x \leq t^\star) = \lim_{n \to \infty} \PP(t_L + \frac{1}{n} \leq x \leq t^\star) \leq \epsilon. \qedhere \]
\end{proof}

\end{document}
