\documentclass{article}
\usepackage{fullpage}
\usepackage{physics}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{commath}
\usepackage{algorithm, algorithmic}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}

\DeclareMathOperator*{\hinge}{{\rm hinge}}
\DeclareMathOperator*{\kl}{{\rm kl}}
\DeclareMathOperator*{\h}{{\rm h}}
\DeclareMathOperator*{\KL}{{\rm KL}}
\DeclareMathOperator*{\nent}{{\rm H}}
\DeclareMathOperator*{\Bin}{{\rm B}}
\DeclareMathOperator{\Rad}{{\mathrm{Rad}}}
\DeclareMathOperator*{\R}{{\rm R}}
\DeclareMathOperator*{\U}{{\rm U}}
\DeclareMathOperator*{\N}{{\rm N}}
\DeclareMathOperator*{\Var}{{\rm Var}}
\DeclareMathOperator*{\err}{{\rm err}}
\DeclareMathOperator*{\sign}{{\rm sign}}
\DeclareMathOperator*{\Xcal}{{\cal X}}
%\DeclareMathOperator*{\Hcal}{{\cal H}}
\DeclareMathOperator*{\Ycal}{{\cal Y}}
\DeclareMathOperator*{\Acal}{{\cal A}}
\DeclareMathOperator*{\Zcal}{{\cal Z}}
\DeclareMathOperator*{\Gcal}{{\cal G}}
%\DeclareMathOperator*{\Fcal}{{\cal F}}
\DeclareMathOperator*{\Scal}{{\cal S}}
\DeclareMathOperator*{\Ical}{{\cal I}}
\DeclareMathOperator*{\argmin}{{\rm argmin}}
\DeclareMathOperator*{\argmax}{{\rm argmax}}
\DeclareMathOperator*{\maximize}{{\rm maximize}}
\DeclareMathOperator*{\minimize}{{\rm minimize}}
\DeclareMathOperator*{\st}{{\rm s.t.}}
\DeclareMathOperator*{\VC}{{\rm VC}}
\DeclareMathOperator*{\EE}{{\mathbb E}}
\DeclareMathOperator*{\PP}{{\mathbb P}}
\newcommand{\RR}{\mathbb{R}} % Real numbers
%\newcommand{\EE}{\mathbb{E}}
%\newcommand{\PP}{\mathbb{P}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\defeq}{\triangleq}
\newcommand*{\one}{{\bf 1}}
\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}
\DeclareMathOperator*{\Ber}{{\rm Bernoulli}}

\title{CSC 665: Support Vector Machines}
\author{Chicheng Zhang}

\begin{document}
\maketitle

\section{Support vector machines - the maximum margin hyperplane problem}
We consider linear classification, where examples $(x_i,y_i)_{i=1}^m$ are such that
$x_i \in \RR^d$ are features, and $y_i \in \cbr{\pm 1}$ are binary labels.

Suppose that the training set $S = (x_i,y_i)_{i=1}^m$ is linearly separable, i.e.
there exists a linear classifier $(w,b) \in \RR^{d+1}$, such that for all $i$,
\begin{equation}
  \begin{cases} \inner{w}{x_i} + b > 0 & y_i = +1, \\ \inner{w}{x_i} + b < 0 & y_i = -1. \end{cases}
    \label{eqn:ls}
  \end{equation}

One way to train a linear classifier would be to use the consistency algorithm, i.e.
solving a linear program, that finds a $(w,b)$ such that Equation~\eqref{eqn:ls} holds.
However, note that not all consistent linear classifiers are created equal: some of them are closer to training examples than others. Formally, the distance of a point $x$ in $\RR^d$ to a hyperplane $H_{w,b} = \cbr{x_0: w \cdot x_0 + b = 0}$ is defined as the shortest distance of $x$ to any of the points in $H_{w,b}$:
\begin{equation}
  d(x, H_{w,b}) = \min\cbr{\|x - x_0\|: w \cdot x + b = 0}.
\end{equation}
Can we calculate this distance analytically? First, let us assume without loss of generality that $\| w \| = 1$, as any hyperplane $H_{w',b'}$ can be written as $H_{w,b}$ for $\| w \| = 1$ by letting $w = \frac{w'}{\|w'\|}$ and $b = \frac{b'}{\|w'\|}$. Now, consider a point $x_0 \in H_{w,b}$ such that $x_0 = x + \alpha w$ for some $\alpha$. What is the value of $\alpha$? Note that
\[ \inner{w}{x + \alpha w} + b = 0, \]
which implies that $\alpha = -(\inner{w}{x} + b)$.
\begin{claim}
For all $x_1$ in $H_{w,b}$,
\begin{equation}
  \| x_1 - x \| \geq \| x_0 - x\|.
  \label{eqn:proj}
\end{equation}
Consequenty, $d(x, H_{w,b}) = |\inner{w}{x} + b|$.
\end{claim}
\begin{proof}
Note that $x_0$ and $x_1$ are both in $H_{w,b}$, $\inner{w}{x_0} + b = \inner{w}{x_1} + b = 0$. Therefore,
$\inner{x_1 - x_0}{w} = 0$. In other words,
\[ \inner{x_1 - x_0}{x_0 - x} = 0. \]
Now, by Pythagorean theorem,
\[ \| x - x_1 \|^2 = \| x - x_0 \|^2 + \| x_0 - x_1 \|^2 \geq \| x - x_0 \|^2,  \]
whch proves Equation~\eqref{eqn:proj}. This implies that
\[ d(x, H_{w,b}) = \| x - x_0 \| = |\inner{w}{x} + b|. \]
\end{proof}

Here is a proposal:
\begin{quote}
  Find the linear classifier $(w,b)$ that not only separates the examples but also maximizes the minimum distances to all examples.
\end{quote}
Why is the proposal sensible? One observation is that this classifier is the most ``robust''. For example, if test examples happen to be just a little distance away from training examples (with the same labels), then this classifier would still classify such examples correctly.

Formally, we can describe the proposal as an optimization problem:
\begin{align}
  \maximize_{w,b,A} \quad & A & \label{eqn:orig} \\
    \st \quad & A > 0, \quad \| w \| = 1, & \nonumber \\
    &  y_i(\inner{w}{x_i} + b) > 0, &\forall i \in \cbr{1,\ldots,n},\nonumber \\
    & |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}. \nonumber
\end{align}

The above program is not a convex program, and is difficult to optimize directly.
Let's make a few transformations to make it a convex program - i.e. finding a convex optimization problem whose solution is related to that of the above optimization problem.

Let's consider the following optimization problem:
\begin{align}
  \maximize_{w,b,A} \quad & A & \label{eqn:pos} \\
    \st \quad & A > 0, \quad \| w \| = 1, & \nonumber \\
    &  y_i(\inner{w}{x_i} + b) \geq A, &\forall i \in \cbr{1,\ldots,n}, \nonumber
    %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
\end{align}
Our claim is that the above two optimization problems have the same solutions. Why?
Because under $A > 0$, constraints $y_i(\inner{w}{x_i} + b) > 0$ and $|\inner{w}{x_i} + b| \geq A$, together, are equivalent to $y_i(\inner{w}{x_i} + b) \geq A$, as $y_i \in \cbr{\pm 1}$. For every $i$, the quantity $y_i(\inner{w}{x_i} + b)$ is the {\em margin} of halfspace $H_{w,b}$ on example $(x_i, y_i)$.
Therefore the above is also called the ``maximum margin hyperplane'' problem.

Now let $w' = \frac{w}{A}$, $b' = \frac{b}{A}$. Note that the above optimization
problem is equivalent to
\begin{align}
  \maximize_{w',b',A} \quad & A & \nonumber \\
    \st \quad & A > 0, \quad \| w' \| = \frac{1}{A}, & \nonumber \\
    &  y_i(\inner{w'}{x_i} + b') \geq 1, &\forall i \in \cbr{1,\ldots,n}, \nonumber
    %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
\end{align}
Furthermore, this is equivalent to
\begin{align}
  \minimize_{w',b'} \quad & \| w' \| & \nonumber \\
    \st \quad &  y_i(\inner{w'}{x_i} + b') \geq 1, &\forall i \in \cbr{1,\ldots,n}, \nonumber
    %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
\end{align}
As the function $x \mapsto \frac12 x^2$ is monotonically increasing for $x > 0$, we get that the above is
equivalent to
\begin{align}
  \minimize_{w',b'} \quad & \frac12 \| w' \|^2 & \label{eqn:svm} \\
    \st \quad &  y_i(\inner{w'}{x_i} + b') \geq 1, &\forall i \in \cbr{1,\ldots,n}, \nonumber
    %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
\end{align}

Optimization problem~\eqref{eqn:orig} is called the {\em support vector machine} (SVM). Note that its constraints are all linear inequalities, which defines a convex constraint set. In addition, its optimization objective is a quadratic function over optimization variables, which is a convex function. This implies that it is a {\em convex optimization} problem.

\paragraph{Recovering the optimal solution of~\eqref{eqn:orig}.} Suppose we have a solution of~\eqref{eqn:svm}, written as $(w'^\star, b'^\star)$. Note that the optimal $A$ in~\eqref{eqn:pos} (thus, in~\eqref{eqn:orig}) is $1/\| w'^\star \|$, which is the value of the minimum margin.
This implies that in~\eqref{eqn:pos} (thus, in~\eqref{eqn:orig}), $w^\star = A^\star w'^\star = \frac{w'^\star}{\|w'^\star\|}$,
$b^\star = A^\star b'^\star = \frac{b'^\star}{\|w'^\star\|}$.
The optimal hyperplane is simply $H_{w^\star,b^\star} = H_{w'^\star, b'^\star}$.


\subsection{Optimality condition}
To avoid notation clutter, let us drop the apostrophes in optimization problem~\eqref{eqn:svm}:
\begin{align}
  \minimize_{w,b} \quad & \frac12 \| w \|^2 & \label{eqn:svm-d} \\
    \st \quad &  y_i(\inner{w}{x_i} + b) \geq 1, &\forall i \in \cbr{1,\ldots,n}, \nonumber
\end{align}
What property does the optimal solution $(w^\star, b^\star)$ have? We will take a detour and first discuss Lagrangian duality, a fundamental concept in constrained optimization. Let us first write \eqref{eqn:svm-d} as an unconstrained optimization problem over a slightly more complicated objective:
\begin{equation}
   \min_{w,b} \max_{\alpha \geq 0} L(w,b,\xi;\alpha),
   \label{eqn:svm-lag}
\end{equation}
where $L(w,b;\alpha) = \frac 1 2 \| w \|^2 + \sum_{i=1}^n \alpha_i (1 - y_i(\inner{w}{x_i} + b))$.

Define $P(w,b) := \max_{\alpha \geq 0} L(w,b;\alpha)$. Observe that:
\[
P(w,b) =
\begin{cases}
+\infty, & \exists i, 1 - y_i(\inner{w}{x_i} + b) > 0 \\
\frac 1 2 \| w \|^2, & \forall i, 1 - y_i(\inner{w}{x_i} + b) \leq 0
\end{cases}
\]
Therefore, optimization problem~\eqref{eqn:svm-lag} is equivalent to \eqref{eqn:svm-d}. Now consider switching the orders of min and max in \eqref{eqn:svm-lag}:
\[ \max_{\alpha \geq 0} \min_{w,b} L(w,b;\alpha). \]
This is called the dual problem of \eqref{eqn:svm-lag} (\eqref{eqn:svm-lag} is called the primal problem). Let's call the optimal primal value $p^\star$ and the optimal dual value $d^\star$. What's the relationship between the primal and dual problems, and their respective optimal solutions?
%It is well known that for general function $L$, $p^\star \geq d^\star$. However,
%under fairly general assumptions, it can also be shown that $p^\star = d^\star$.

We state the following result from numerical optimization. Consider a constrained convex optimization problem that has both equality and inequality constraints:
\begin{align}
  \minimize_{x} \quad & f(x) & \nonumber \\
    \st \quad & g_i(x) \leq 0, &\forall i \in \cbr{1,\ldots,n}, \nonumber \\
              & h_i(x) = 0, &\forall i \in \cbr{1,\ldots,m}. \nonumber
\end{align}
Similar as before, we can define Lagrange function $L(x,\alpha,\beta) = f(x) + \sum_{i=1}^n \alpha_i g_i(x) + \sum_{i=1}^m \beta_i h_i(x)$. Define
\[ P(x) \defeq \max_{\alpha \geq 0, \beta} L(x,\alpha,\beta), \]
\[ D(\alpha, \beta) = \min_x L(x,\alpha,\beta), \]
\[ p^\star = \min_x P(x) = \min_x \max_{\alpha \geq 0, \beta} L(x,\alpha,\beta),\]
\[ d^\star = \max_{\alpha \geq 0, \beta} D(\alpha, \beta) = \max_{\alpha \geq 0, \beta} \min_x L(x,\alpha,\beta),\]
we have the following result.

\begin{theorem}
  Under mild assumptions\footnote{specifically, $f$, $g_i$'s are convex, $h_i$'s are linear, and there exists $w,b,\xi$ such that all inequality constraints in are stictly satisfied, namely the Slater condition.}, we have that there exists $x^\star$, $\alpha^\star$, and $\beta^\star$, such that
  \begin{enumerate}
  \item $x^\star$ is optimal solution of the primal problem and $\alpha^\star, \beta^\star$ is the optimal solution of the dual problem.
  \item Strong duality holds:
  \[ p^\star = L(x^\star, \alpha^\star, \beta^\star) = d^\star. \]
  \item Karush-Kuhn-Tucker (KKT) condition holds:
  \begin{align*}
    &\nabla_x L(x^\star,\alpha^\star,\beta^\star) = 0, && \text{Stationarity}\\
    \forall i, \quad &g_i(x^\star) \leq 0, h_i(x^\star) \leq 0, && \text{Primal feasible} \\
    \forall i, \quad &\alpha_i \geq 0, && \text{Dual feasible} \\
    \forall i, \quad &\alpha_i g_i(x^\star) = 0. && \text{Complementary slackness}
  \end{align*}
\end{enumerate}
\end{theorem}
%\paragraph{Remark.} The above theorem also (implicitly gives) relationship between primal and dual optimal solutions. We claim that $x^\star$ (resp. $\alpha^\star, \beta^\star$) is the optimal solution for the primal (resp. dual) problem. To see why, note that both $x^\star$ and $\alpha^\star, \beta^\star$ are feasible in the respective optimization problems. In addition, as
%\[ L(x^\star, \alpha^\star, \beta^\star) = p^\star,  \]
%we immediately have
%\[ P(x^\star) = \max_{\alpha \geq 0,\beta} L(x^\star, \alpha, \beta) \geq L(x^\star, \alpha^\star, \beta^\star) p^\star,  \]

Applying the theorem to SVM optimization, we can also recover the primal optimal solution $(w^\star, b^\star)$ from dual solution $\alpha^\star$ by invoking the KKT condition. To see why, recall that in SVM, $L(w,b;\alpha) = \frac \lambda 2 \| w \|^2 + \sum_{i=1}^n \alpha_i (1 - y_i(\inner{w}{x_i} + b))$, hence by stationarity condition,
\[ \nabla_w L(w,b;\alpha) = \lambda w - \sum_{i=1}^n \alpha_i y_i x_i = 0, \]
which implies that
\[ w^\star = \sum_{i=1}^n \alpha_i^\star y_i x_i. \]
that is, the optimal solution is a linear combination of the feature vectors of training exmaples.

Furthermore, denote by $\Ical = \cbr{i: y_i(\inner{w^\star}{x_i} + b^\star = 1}$ the set of examples that has margin exactly equal to 1.
Complementary slackness says that for all $i$,
\[ \alpha_i^\star (1 - y_i(\inner{w^\star}{x_i} + b^\star)) = 0. \]
This implies that for an $i \notin \Ical$, as $y_i(\inner{w^\star}{x_i} + b^\star > 1$, $\alpha_i = 0$. We call $\Ical$ the set of {\em support vectors}, which are the vectors that ``contribute'' to the optimal solution $w^\star$.

It can also be verified that there exists at least one $i$, $y_i(\inner{w^\star}{x_i} + b^\star) = 1$. Pick one such $i$; $b^\star$ can be recovered by the formula $b^\star = y_i - \inner{w^\star}{x_i}$.


%we immediately have that the dual optimization problem has the same optimal value as the primal optimization problem.
%In addition, in SVM,
%How does the dual optimal solution relate to
%optimal solution of \eqref{eqn:svm}:


%We invoke a fundamental result from optimization.
%\begin{lemma}[Fritz John optimiality condition]
%Let $x^\star$ be the solution of
%\begin{align}
%  \minimize_{x} \quad & f(x) & \nonumber \\
%    \st \quad & g_i(x) \leq 0, &\forall i \in \cbr{1,\ldots,n}, \nonumber
%\end{align}
%Then there exists $\alpha_1, \ldots, \alpha_m \geq 0$, such that
%\[ \nabla f(x^\star) + \sum_{i \in \Ical} \alpha_i \nabla g_i(x^\star) = 0, \]
%where $\Ical = \cbr{i \in \cbr{1,\ldots,n}: g_i(x^\star) = 0}$.
%\end{lemma}
%Applying the lemma to~\eqref{eqn:svm-d}, we get that there exists $\alpha_1, \ldots, \alpha_m \geq 0$, such that
%\[ w^\star + \sum_{i \in \Ical} \alpha_i y_i x_i = 0, \]
%where $\Ical = \cbr{i \in \cbr{1,\ldots,n}: y_i (\inner{w}{x_i} + b) = 1}$. We call the set $\cbr{(x_i, y_i): i \in \Ical}$ the {\em support vectors}, as the optimal solution can be written as a linear combination of them, and these points has the lowest margin (all equal to 1).

\subsection{Coping with linear non-spearability}
Can we still train SVM if the data is not linearly separable? Note that optimization problem~\eqref{eqn:svm} will not find a solution, as now the constraint set become infeasible. Generally there are two ways to sidestep this problem: first, introduce nonlinear feature maps; second, relax the SVM formulation to allow for training examples
to be classified incorrectly.

For the first approach, we can consider having a feature map $\phi: \RR^d \to \RR^m$, so that every example $(x_i, y_i)$ is transformed to $(\phi(x_i), y_i)$. Suppose $(\phi(x_i), y_i)$ is linearly separable, then we compute a SVM over these examples to get $w^\star \in \RR^{m+1}$ and $b \in \RR$. Our output linear classifier is $\sign(\inner{w^\star}{\phi(x_i)} + b^\star)$.
For example, suppose we have a distribution $D$ over $\RR^2 \times \cbr{\pm 1}$ such that for all examples $(x, y)$'s on the support of $D$, $x_1^2 + x_2^2 \leq 1 \Leftrightarrow y = +1$. In this case, we can introduce feature map $\phi(x) = (x_1^2, x_2^2)$ to make the dataset linearly separable.

For the second approach, we introduce slack variables $\xi_i \geq 0$ for every example $i$, to allow some example to be misclassified. In addition, we introduce a regularization parameter
$\lambda > 0$ that trades off misclassification and margin on correct examples:
\begin{align}
  \minimize_{w,b,\xi} \quad & \frac \lambda 2 \| w \|^2 + \sum_{i=1}^n \xi_i & \label{eqn:svm-sm} \\
    \st \quad &  y_i(\inner{w}{x_i} + b) \geq 1 - \xi_i, &\forall i \in \cbr{1,\ldots,n}, \nonumber \\
    &  \xi_i \geq 0, &\forall i \in \cbr{1,\ldots,n}, \nonumber
    %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
\end{align}
Intuitively, when $\lambda$ is larger, it focuses more on enforcing large margin on correct examples; when $\lambda$ is smaller, it forces more on reducing misclassification.
Notice that the last two lines can be summarized by: $\forall i \in \cbr{1,\ldots,n}$, $\xi_i \geq \max(0, 1-y_i(\inner{w}{x_i} + b))$. Therefore, the optimal choice of $\xi_i$ equals $\max(0, 1-y_i(\inner{w}{x_i} + b))$.
Let $\phi(z) = \max(0,1-z)$ and $R(w) = \frac{\lambda}{2}\| w \|^2$. We thus can rewrite optimization problem \eqref{eqn:svm-sm} as:
\begin{equation}
    \minimize_{w,b} \quad \lambda R(w)  + \sum_{i=1}^n \phi(y_i(\inner{w}{x_i} + b)).
\end{equation}
As a convention, we call $\phi(y(\inner{w}{x} + b))$ the {\em hinge loss} of linear classifier $(w,b)$ on example $(x,y)$, written as $\ell_{\hinge}((w,b), (x,y))$. When the margin $y(\inner{w}{x} + b)$ is larger, the hinge loss is smaller. The above form is also called a {\em regularized loss minimzation} formulation, which captures a wide range of optimization problems in machine learning (by changing loss function $\phi$ and regularizer $R$), such as logistic regression, ridge regression, lasso, etc.

Both approaches has its own advantages and drawbacks. For the feature transformation approach, it is unclear if a $\phi$ will guarantee that the transformed dataset satisfies linear separability. For the soft margin approach, if the dataset is highly linearly nonpseparable (e.g. the unit circle example discussed above), then as it is still learning a linear classifier, it will not perform well. It may be a good idea to combine nonlinear feature map with soft margin in practice.

\section{The dual of SVM}
Sometimes looking at the dual problem will yield unexpected insights about the original (primal) problem. Indeed, SVM is a canonical example for this statement - we have already seen that the KKT condition implies that we can write the optimal solution $w^\star$ in terms of dual optimal solution $\alpha^\star$. We have discussed the dual problem in an abstract way so far. But what exactly is the dual problem for SVM?

Let us first calculate the dual objective function $D(\alpha) = \min_{w,b} L(w,b;\alpha)$, where
\[ L(w,b;\alpha) = \frac 1 2 \| w \|^2 + \sum_{i=1}^n \alpha_i (1 - y_i(\inner{w}{x_i} + b)). \]
We can write $D(\alpha)$ as follows:
\[ D(\alpha) = \sum_{i=1}^n \alpha_i + \min_w\del{\frac 1 2 \| w \|^2 - \inner{w}{\sum_{i=1}^n \alpha_i y_i x_i}} + \min_b \del{\sum_{i=1}^n \alpha_i y_i b}. \]
Define $g(z) = \begin{cases} -\infty & z = 0 \\ 0 & z \neq 0 \end{cases}$, then
\[ D(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2}\| \sum_{i=1}^n \alpha_i y_i x_i \|^2 + g(\sum_{i=1}^n \alpha_i y_i). \]
Therefore, $\max_{\alpha \geq 0} D(\alpha)$ is equivalent to
\begin{align}
  \maximize_{\alpha} \quad & \sum_{i=1}^n \alpha_i - \frac{1}{2} \|\sum_{i=1}^n \alpha_i y_i x_i\|^2 & \label{eqn:svm-dual-unexp} \\
    \st \quad & \sum_{i=1}^n \alpha_i y_i = 0, \nonumber \\
     & \alpha_i \geq 0, &\forall i \in \cbr{1,\ldots,n}, \nonumber
\end{align}
writing the objective more explicitly, it is
\begin{align}
  \maximize_{\alpha} \quad & \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n y_i y_j \inner{x_i}{x_j} \alpha_i \alpha_j & \label{eqn:svm-dual} \\
   \st \quad & \sum_{i=1}^n \alpha_i y_i = 0, \nonumber \\
     &  \alpha_i \geq 0, &\forall i \in \cbr{1,\ldots,n}, \nonumber
\end{align}
Compared to \eqref{eqn:svm}, this is also a quadratic program, however, its objective becomes a complicated quadratic function, and its constraints says that $\alpha$ lies in the positive orthant of $\RR^n$, which is simpler than the linear inequality constraints in \eqref{eqn:svm}.
%Let's consider writing the constrained optimization problem \eqref{eqn:svm-sm} in the following alternative way:
%\begin{align}
%   \minimize_{w,b,\xi} \max_{\alpha \geq 0, \beta \geq 0} \frac \lambda 2 \| w \|^2 + \sum_{i=1}^n \xi_i + \sum_{i=1}^n \alpha_i (1 - \xi_i - y_i(\inner{w}{x_i} + b)) + \sum_{i=1}^n \beta_i (-\xi_i).
    %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
%\end{align}
%The $\alpha_i$, $\beta_i$'s are called Lagrangian multipliers. Why is the above equivalent to the original soft margin SVM?

%Define $F(w,b,\xi) = \max_{\alpha \geq 0, \beta \geq 0} \frac \lambda 2 \| w \|^2 + \sum_{i=1}^n \xi_i + \sum_{i=1}^n \alpha_i (1 - \xi_i - y_i(\inner{w}{x_i} + b)) + \sum_{i=1}^n \beta_i (-\xi_i)$. Observe that
%\[
%F(w,b,\xi) =
%\begin{cases}
%  -\infty & \exists i, y_i(\inner{w}{x_i} + b)) < 1 - \xi_i \\
%  -\infty & \exists i, \xi_i < 0 \\
%  \frac \lambda 2 \| w \|^2 + \sum_{i=1}^n \xi_i & \text{otherwise}.
%\end{cases}
%\]

%Every constrained convex optimization problem has a dual optimization problem. Sometimes looking at the dual problem will yield unexpected insights about the original (primal) problem! Indeed, SVM is a canonical example for this claim.

%We consider the soft-margin SVM formulation \eqref{eqn:svm-sm}. To derive its dual problem, we introduce dual variables (aka Lagrange multipliers) to incorporate all constraints to the objective function:
%\[
%\min_{w,b,\xi \geq 0} \max_{\alpha \geq 0, \beta \geq 0} \del{\frac \lambda 2 \| w \|^2 + \sum_{i=1}^n \xi_i + \sum_{i=1}^n \alpha_i \cdot (1 - \xi_i - y_i(\inner{w}{x_i} + b)) - \sum_{i=1}^n \beta_i \xi_i}
%\]

%It can be shown~ that the above is equivalent to
%\[
%\max_{\alpha \geq 0, \beta \geq 0} \min_{w,b,\xi \geq 0} \del{\frac \lambda 2 \| w \|^2 + \sum_{i=1}^n \xi_i + \sum_{i=1}^n \alpha_i \cdot (1 - \xi_i - y_i(\inner{w}{x_i} + b)) - \sum_{i=1}^n \beta_i \xi_i}
%\]
%Note that the inner minimization can be written as follows:
%\[ \sum_{i=1}^n \alpha_i + \min_{w} \del{ \frac \lambda 2 \| w \|^2 - \inner{w}{\sum_{i=1}^n \alpha_i y_i x_i}} + \min_{b} (-b \cdot \sum_{i=1}^n y_i \alpha_i) + \sum_{i=1}^n \min_{\xi_i \geq 0} \xi_i(1-\alpha_i-\beta_i) \]

%Let us denote by $h(z) = \begin{cases} -\infty & z < 0 \\ 0 & z \geq 0 \end{cases}$
%and $g(z) = \begin{cases} -\infty & z = 0 \\ 0 & z \neq 0 \end{cases}$. The optimal value of the inner optimization problem is
%\[ \sum_{i=1}^n \alpha_i - \frac{1}{2\lambda} \|\sum_{i=1}^n \alpha_i y_i x_i\|^2 + g(\sum_{i=1}^n y_i \alpha_i) + \sum_{i=1}^n h(1-\alpha_i-\beta_i). \]

%Therefore, the dual problem is equivalent to:
%\begin{align}
%  \maximize_{\alpha, \beta} \quad & \sum_{i=1}^n \alpha_i - \frac{1}{2\lambda} \|\sum_{i=1}^n \alpha_i y_i x_i\|^2 & \label{eqn:svm-dual} \\
%    \st \quad &  \alpha_i + \beta_i \leq 1, &\forall i \in \cbr{1,\ldots,n}, \nonumber \\
%    &  \sum_{i=1}^n y_i \alpha_i = 0, &\forall i \in \cbr{1,\ldots,n}, \nonumber
    %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
%\end{align}
%Moreover, the solutions of the s

\section{The kernel trick}
The dual of SVM \eqref{eqn:svm-dual} uncovers an interesting fact: if we would like to compute the optimal solution of \eqref{eqn:svm}, it suffices to solve the dual optimization problem, whose
objective function only depends on the pairwise inner product between training examples (as opposed to the original feature vectors of training examples).

This opens up a new opportunity: suppose we have a feature map that is extremely high dimensional (say has dimensionality $M$) but has succinct representation on pairwise inner product $\inner{\phi(x)}{\phi(x')}$ (say can be evaluated with time $m$), then we may avoid paying a time complexity of $M$ in learning the SVM classifier on the transformed examples.
Here is the full proposal:
\begin{enumerate}
\item Define $k(x,x') = \inner{\phi(x)}{\phi(x')}$ be the {\em kernel function} associated with feature mapping $\phi$.
\item Solve the dual optimization problem \eqref{eqn:svm-dual}, get $(\alpha_i)_{i=1}^m$.
\item By KKT condition, we can recover
\[ w^\star = \sum_{i=1}^n \alpha_i y_i \phi(x_i), \]
but we only store $w^\star$ {\em implicitly}, i.e. storing the value of all $\alpha_i$'s.
\item To recover $b^\star$, find an $j$ such that $\alpha_j > 0$, and let
\[ b^\star = y_j - \inner{w^\star}{x_j} = y_j - \sum_{i=1}^n \alpha_i^\star y_i k(x_i, x_j), \]
where we directly evaluate $k(x_i, x_j)$ as opposed to calculating $\phi(x_i)$, $\phi(x_j)$ and take their inner product.
\item To make prediction on future example $x$, we compute
\[ \inner{w^\star}{\phi(x)} + b^\star = \sum_{i=1}^n \alpha_i^\star k(x_i, x) + b^\star. \]
Same as before, we directly evaluate the kernel function.
\end{enumerate}

As discussed before, each feature map corresponds to a kernel function. Some feature map gives succinct kernel functions, whereas others may not. For example, for input domain $\RR^2$, define $\phi(x) = (x_1^2, \sqrt{2} x_1 x_2, x_2^2)$. It can be checked that its associated kernel function has a succinct form:
\[ \inner{\phi(x)}{\phi(x')} = (x_1x_1' + x_2x_2')^2 = (\inner{x}{x'})^2. \]
However, if we define $\phi(x) = (x_1^2, x_1 x_2, x_2^2)$, then its corresponding
$k(x,x')$ does not have a succinct form.

Basic properties of kernel functions:
\begin{enumerate}
\item if $K$ is the kernel function of $\phi$, then for positive $c$, $cK$ is the kernel function of $\sqrt{c} \phi$.
\item if $K_1$ (resp. $K_2$) is the kernel function of $\phi_1$ (resp. $\phi_2$), then $K_1 + K_2$ is the kernel function of $\phi(x) = (\phi_1(x), \phi_2(x))$.
\item if $K_1$ (resp. $K_2$) is the kernel function of $\phi_1$ (resp. $\phi_2$), then $K_1 \cdot K_2$ is the kernel function of $\phi(x) = \phi_1(x) \otimes \phi_2(x)$, where the $\otimes$ notation denotes the Kronecker product. Suppose $a = (a_1,\ldots,a_n)$ and $b = (b_1,\ldots,b_m)$. Then,
\[ a \otimes b = \begin{bmatrix} a_1 b \\ \ldots \\ a_n b \end{bmatrix} = \begin{bmatrix} a_1 b_1\\ \ldots \\ a_1 b_m \\ \ldots \\ a_n b_1 \\ \ldots \\ a_n b_m \end{bmatrix}. \]
The claim follow from a basic fact about Kronecker product:
\[ \inner{a \otimes b}{c \otimes d} = \inner{a}{c} \cdot \inner{b}{d}. \]
\item if $K_1$ is the kernel function of $\phi_1$, and $f$ is an arbitrary scalar function, then $K_2(x,x') = K_1(x,x') f(x) f(x')$ is the kernel function of $\phi_2(x) = f(x) \phi_1(x)$.
\end{enumerate}

Examples of kernel functions:
\begin{enumerate}
\item Linear kernel $K_1(x,x') = (1 + \sum_{i=1}^d x_i x_i') = 1 + \inner{x}{x'}$. Define feature map $\phi_1(x) \defeq (1, x_1, \ldots, x_d)$. It can be checked that $k_1(x,x') = \inner{\phi_1(x)}{\phi(x')}$.

\item Polynomial kernel $K_2(x,x') = (1 + \inner{x}{x'})^s$ for $s \geq 1$.
Then define $\phi_2(x) \defeq \phi_1(x)^{\otimes s} = \phi_1(x) \otimes \ldots \otimes \phi_1(x)$. It can be checked by property of Kronecker product that $\inner{\phi_2(x)}{\phi_2(x')} = (\inner{\phi_1(x)}{\phi_1(x')})^s = k_2(x,x')$.

%\item Exponential kernel $k(x,x') = \exp$

\item Radial basis function (RBF) kernel $K_3(x,x') = \exp(-\frac{\|x-x'\|^2}{2\sigma^2})$ for $\sigma > 0$.
First, note that $K_3(x,x') = \exp(-\frac{\|x\|^2}{2\sigma^2}) \cdot \exp(-\frac{\|x'\|^2}{2\sigma^2}) \cdot K_4(x,x')$, where
\[ K_4(x,x') = \exp(\frac{\inner{x}{x'}}{\sigma^2}) = \sum_{i=0}^\infty \frac{(\inner{x}{x'})^i}{\sigma^{2i} i!}. \]
Let us define $\phi_4(x) = \del{ \frac{1}{\sigma^i \sqrt{i!}} x^{\otimes i} }_{i=0}^\infty$; it can be easily seen that it is the feature map of $K_4$.
Therefore, $\phi_3(x) = \del{ \exp(-\frac{\|x\|^2}{2\sigma^2}) \cdot \frac{1}{\sigma^i \sqrt{i!}} x^{\otimes i} }_{i=0}^\infty$ is the feature map of $K_3$. Note that this is an example of a kernel with infinite dimensional feature map, and the primal SVM problem \ref{eqn:svm} cannot even be explicitly written down.

\item String kernel. Suppose the strings are over a finite alphabet $\Sigma$ (e.g. $\Sigma = \cbr{A,T,C,G}$ for DNA sequences). For two strings $s$ and $s'$, define $K_5(s,s') = \abs{\cbr{t \in \Sigma^\star: t \text{ is a common substring of } s \text{ and } s'}}$. Define feature map $\phi_5(s) = (\one(t \text{  is a substring of } s))_{t \in \Sigma^\star}$.
It can be checked that $K_5(s,s') = \inner{\phi_5(x)}{\phi_5(x')}$. This is also an example of a kernel with infinite dimensional feature map, and moreover the input domain is the set of strings as opposed to the familiar Euclidean space.
\end{enumerate}


\section{Margin bounds for linear classification - why does SVM work well?}
Recall that in PAC learning, we have seen that, given a distribution $D$ over $\RR^d \times \cbr{\pm 1}$ realizable by the set of linear classifiers, any consistent classifier will have
 an error rate of $O(\frac{d\ln\frac{m}{d}}{m})$ with high probability.
 Note that
the generalization bound depends crucially on the dimensionality of the data.
 However, it has been observed that SVM works quite well in practice, even if it uses
 a function kernel whose feature map is extremely-high (or even, infinite) dimensional. What is going on in SVM that makes it effective?

 In this section, we give evidence sheding lights on the effecitiveness of
  SVM in practice. For simplicity, we only consider SVM for homogeneous linear classifiers, that is
  \begin{align}
    \minimize_{w'} \quad & \frac12 \| w' \|^2 & \label{eqn:svm-h} \\
      \st \quad &  y_i \inner{w'}{x_i} \geq 1, &\forall i \in \cbr{1,\ldots,n}, \nonumber
      %& |\inner{w}{x_i} + b| \geq A, & \forall i \in \cbr{1,\ldots,n}.
  \end{align}
  We show the following theorem.

\begin{theorem}
Fix $B, R > 0$. Suppose $S$ is a set of examples $(x_i,y_i)_{i=1}^n$ drawn iid from distribution $D$ on $\cbr{x \in \RR^d: \| x \|_2 \leq R} \times \cbr{\pm 1}$.
Then with probability $1-\delta$, for all classifiers $w \in \cbr{w: \|w\|_2 \leq B}$, and all margin parameters
$\gamma \in (0, BR]$, we have
\[
\PP_D( y \inner{w}{x} \leq 0 )
\leq \PP_S (y \inner{w}{x} < \gamma) + \frac{BR}{\gamma} \sqrt{\frac{8 + 4\ln(\frac{2}{\delta}) + 2\ln(1+\log_2(\frac{BR}{\gamma}))}{m}}.
\]
\label{thm:mb-l2}
\end{theorem}

The theorem is called a ``margin bound'', in the sense that the generalization error bound of the classifier depends on two quantities: first, the empirical ``margin error'' of the classifier, where an example is counted as error when it has a margin smaller than $\gamma$; second, a concentration term that decreases with margin $\gamma$. Importantly, the theorem is {\em dimension-free} - it holds as long as examples and linear predictors have bounded norm.

Another feature in the above statement is that it is invariant under positive scaling of $B$ and $\gamma$: consider a positive number $\alpha$. Given a $w$ such that $\| w \|_2 \leq B$ and a margin $\gamma \in (0, BR]$, consider their scaling $w' = \alpha w$ (with norm at most $B' = \alpha B$)  and $\gamma' \in (0,\alpha BR]$. Then we have the following identities on events
\[ \cbr{y \inner{w}{x} \leq 0} = \cbr{y \inner{w'}{x} \leq x },
\quad \cbr{y \inner{w}{x} \leq \gamma} = \cbr{y \inner{w'}{x} \leq \gamma'}. \]
In addition, the generalization bounds depends only on $B/\gamma$, which is equal
to $B'/\gamma'$. This implies that it is impossible to ``game'' the theorem by initially obtaining a statement with some alternative value of $B$ and use the above scaling reasoning to get a sharper margin bound.

%to see this, note that given a scalar $\alpha > 0$, the fact that
%\[
%\PP_D( y \inner{w}{x} \leq 0 )
%\leq \PP_S (y \inner{w}{x} \leq \gamma) + \frac{BR}{\gamma} \sqrt{\frac{\ln2/\delta}{m}}, \forall w,\gamma: \| w \| \leq B, \gamma \in (0, BR].
%\]
%is equivalent to
%\[
%\PP_D( y \inner{w}{x} \leq 0 )
%\leq \PP_S (y \inner{w}{x} \leq \gamma) + \frac{R}{\gamma} \sqrt{\frac{\ln2/\delta}{m}}, \forall w,\gamma: \| w \| \leq 1, \gamma \in (0, R].
%\]
By this theorem, we immediately have the following
important consequence regarding SVM.
\begin{corollary}
Same setting as above. Suppose $w^\star$ is such that %$\| w^\star \| \leq B^\star$ and
\[ \PP_{D}(y \inner{w^\star}{x} \geq \gamma) = 1. \]
Then, with probability $1-\delta$, SVM returns a classifier $\hat{w}$, such that
\[ \PP_D( y \inner{\hat{w}}{x} \leq 0 )
   \leq
   \frac{\|w^\star\|R}{\gamma} \sqrt{\frac{8 + 4\ln(\frac{2}{\delta}) + 2\ln(1+\log_2(\frac{\|w^\star\|R}{\gamma}))}{m}}.
 \]
\end{corollary}
When $\frac{\|w^\star\|R}{\gamma} \ll d$, this theorem provides much stronger guarantees than $O(\frac{d}{m})$ generalization error bound as guaranteed by VC theory.
In addition, using advance techniques, one can show that the generalization bound is in fact $O(\frac{\|w^\star\|^2R^2}{\gamma^2 m})$; therefore, so long as $\frac{\|w^\star\|^2R^2}{\gamma^2}$ is smaller than $d$, this provides more favorable guarantees.

\begin{proof}
Note that $w^\star / \gamma$ is a feasible solution of \ref{eqn:svm-h}. By the optimality of $\hat{w}$, we know that $\| \hat{w} \| \leq \| w^\star / \gamma \|$. Now consider vector $w' = \gamma \hat{w}$.
It can be seen that $\PP_D( y \inner{w}{x} \leq 0 ) = \PP_D( y \inner{w'}{x} \leq 0 )$, $\| w' \| \leq \| w^\star \|$ and for all $i$, $y_i \inner{w'}{x_i} \geq \gamma$.

Now, applying Theorem~\ref{thm:mb-l2} with $w = w'$, along with
$B = \| w^\star \|$ and $\gamma$, we have that
%. It can be readily seen that
\[ \PP_D( y \inner{w'}{x} \leq 0 )
\leq \PP_S (y \inner{w'}{x} < \gamma) + \frac{\|w^\star\|R}{\gamma} \sqrt{\frac{8 + 4\ln(\frac{2}{\delta}) + 2\ln(1+\log_2(\frac{\|w^\star\|R}{\gamma}))}{m}}. \]
As $\PP_S (y \inner{w'}{x} < \gamma) = 0$, we immediately have the theorem statement.
\end{proof}

The proof of Theorem~\ref{thm:mb-l2} is slightly involved. It will be based on the following two key steps: first, relating 0-1 error and margin error to a new loss function named ramp loss; second, conduct a Rademacher complexity-based analysis of the ramp loss class, through the way we will also develop general tools bounding the Rademacher complexity of function classes.

\paragraph{Step 1: relating 0-1 error to ramp loss.} Define the ramp loss as follows:
$\ell_\gamma(w, (x,y)) = \phi_\gamma(y \inner{w}{x})$, where
\[
  \phi_\gamma(z) = \begin{cases} 1,& z \leq 0, \\ 1 - \frac{z}{\gamma}, & 0 < z < \gamma, \\ 0, & z \geq \gamma. \end{cases}
\]
Observe that $\one(z \leq 0) \leq \phi_\gamma(z) \leq \one(z \leq \gamma)$. In addition, $\phi_\gamma$ is $\frac1\gamma$-Lipschitz. Therefore,
\[
   \PP_{(x,y) \sim \Delta}(y \inner{w}{x} \leq 0)
   \leq \EE_{(x,y) \sim \Delta} \phi_\gamma(y \inner{w}{x})
   \leq \PP_{(x,y) \sim \Delta}(y \inner{w}{x} \leq \gamma).
\]
For both $\Delta$ = $\U(S)$ or $D$.

Therefore, it suffices to show the following theorem:
\begin{theorem}
Suppose function $\phi$ is $L$-Lipschitz. Then with probability $1-\delta'$, for all $w$ such that $\| w \| \leq B$,
\[
  \EE_{(x,y) \sim D} \phi(y \inner{w}{x})
  \leq \EE_{(x,y) \sim S} \phi(y \inner{w}{x})
  + L B R \sqrt{\frac{8 + 4\ln(\frac{2}{\delta'})}{m}}.
\]
\label{thm:lip-uc}
\end{theorem}

Why does this imply the original theorem statement?
fix $\gamma \in (0, BR]$, consider $\phi = \phi_\gamma$ which is $\frac 1 \gamma$-Lipschitz. Then,
with probability $1-\delta'$, for all $w$ such that $\| w \| \leq B$,
\[
  \EE_{(x,y) \sim D} \phi_\gamma(y \inner{w}{x})
  \leq \EE_{(x,y) \sim S} \phi_\gamma(y \inner{w}{x})
  + \frac{BR}{\gamma} \sqrt{\frac{8 + 4\ln(\frac{2}{\delta'})}{m}}.
\]

Consider the above statement
with $\gamma_i = \frac{BR}{2^i}$ and $\delta_i = \frac{\delta}{2i^2}$ for $i = 1,2,\ldots$. Then by a union bound, with probability $1-\delta$, for all $w$
such that $\| w \| \leq B$, and all $i \in \NN_+$,
\[
  \EE_{(x,y) \sim D} \phi_{\gamma_i}(y \inner{w}{x})
  \leq \EE_{(x,y) \sim S} \phi_{\gamma_i}(y \inner{w}{x})
  + \frac{BR}{\gamma_i} \sqrt{\frac{8 + 4\ln(\frac{2}{\delta_i})}{m}}.
\]
This implies that for all $i$ in $\NN_+$,
\[
  \PP_{(x,y) \sim D}(y \inner{w}{x} \leq 0)
  \leq \PP_{(x,y) \sim S}(y \inner{w}{x} \leq \gamma_i) + \frac{BR}{\gamma_i} \sqrt{\frac{8 + 4\ln(\frac{2}{\delta_i})}{m}}.
\]
Now consider a general $\gamma \in (0,BR]$. Note that we can always find a
$i$ in $\NN_+$, such that $\gamma_i < \gamma \leq 2\gamma_i$. For this choice of $i$,
we have that $\frac{1}{\gamma_i} \leq \frac{2}{\gamma}$, implying $i \leq 1 + \log_2(\frac{BR}{\gamma})$; in addition,
\[ \PP(y\inner{w}{x} \leq \gamma_i) \leq \PP(y\inner{w}{x} < \gamma). \]
Therefore, for all $\gamma \in (0, BR]$,
\[
\PP_{(x,y) \sim D}(y \inner{w}{x} \leq 0)
\leq \PP_{(x,y) \sim S}(y \inner{w}{x} < \gamma) + \frac{BR}{\gamma} \sqrt{\frac{8 + 4\ln(\frac{2}{\delta}) + 2\ln(1+\log_2(\frac{BR}{\gamma}))}{m}}.
\]

\paragraph{Step 2: The uniform convergence of Lipschitz losses via Rademacher complexity based analysis.} Now our goal comes down to proving Theorem~\ref{thm:lip-uc}. Define loss function class
$\Fcal = \cbr{\ell_{\phi,w}: \|w\| \leq B}$, where $\ell_{\phi,w}(x,y) = \phi(y \inner{w}{x})$ is the $\phi$-loss induced by classifier $w$.
It can be straightforwardly seen that Theorem~\ref{thm:lip-uc} is a statement on the uniform convergence of $\EE_S f(Z)$ to $\EE_D f(Z)$.

By exactly the same reasoning as in the uniform convergence proof (see ``Rademacher complexity'' note), we can easily show that with probability $1-\delta'$,
\begin{equation}
\EE_{(x,y) \sim D} \phi(y \inner{w}{x})
- \EE_{(x,y) \sim S} \phi(y \inner{w}{x})
\leq 2 \Rad_m(\Fcal) + L B R \sqrt{\frac{2\ln(\frac{2}{\delta'})}{m}},
\label{eqn:uc-rad}
\end{equation}
where $\Rad_m(\Fcal) = \EE \Rad_S(\Fcal)$, and
\[
\Rad_S(\Fcal) = \frac{1}{m} \EE_{\sigma_1,\ldots,\sigma_m \sim \R} \sup_{f \in \Fcal}
\sum_{i=1}^m \sigma_i f(z_i)
\]
are the population and empirical Rademacher complexities respectively.

\paragraph{Remark.} A careful reader may notice that the empirical Rademacher complexity $\Rad_S(\Fcal)$
is defined a bit differently from before; however, the original proof goes through with almost no changes: when we apply McDiarmid's inequality, we need to check the sensitivity of function \[ \sup_{w: \|w\| \leq B} \EE_{(x,y) \sim D} \phi(y \inner{w}{x})- \EE_{(x,y) \sim S} \phi(y \inner{w}{x}), \]
it can be shown that the above function is $\frac{2LBR}{m}$-sensitive. The symmetrization step is also almost identical to before, except that we don't have absolute value operation on the deviation between empirical loss and generalization loss.

Now let us look at $\Rad_m(\Fcal)$ more closely; first recall that $\Rad_m(\Fcal) = \EE_{S \sim D^m} \Rad_S(\Fcal)$, where
\[ \Rad_S(\Fcal) = \frac{1}{m} \EE_{\sigma_1,\ldots,\sigma_m \sim \R} \sup_{w: \|w\|_2 \leq B} \sum_{i=1}^m \sigma_i \phi(y_i \inner{w}{x_i}). \]
Also, note that $\Fcal$ can be written as the following composite class: $\Fcal = \cbr{\phi \circ g: g \in \Gcal}$, where $\Gcal = \cbr{ m_w: \| w \| \leq B }$, where
$m_w(x,y) = y\inner{w}{x}$ is the margin function. We have the following lemma that relates the Rademacher complexity of $\Fcal$ and that of $\Gcal$.

\begin{lemma}[Contraction Lemma]
Suppose $S = \cbr{z_1,\ldots,z_m}$ is a dataset of size $m$. In addition, suppse $\Gcal$ is a function class, and $\phi$ is an $L$-Lipschitz function. Then, define $\Fcal = \cbr{\phi \circ g: g \in \Gcal}$, we have:
\[ \Rad_S(\Fcal) \leq L \Rad_S(\Gcal). \]
\end{lemma}

What do we know about $\Rad_S(\Gcal)$?
\begin{eqnarray*}
\Rad_S(\Gcal)
&=& \frac{1}{m} \EE \sup_{w: \|w\|_2 \leq B} \sum_{i=1}^m \sigma_i y_i \inner{w}{x_i} \\
&=& \frac{1}{m} \EE \sup_{w: \|w\|_2 \leq B} \sum_{i=1}^m \sigma_i \inner{w}{x_i} \\
&=& \frac{B}{m} \EE \| \sum_{i=1}^m \sigma_i x_i \| \\
&\leq& \frac{B}{m} \sqrt{\EE \| \sum_{i=1}^m \sigma_i x_i \|^2} \\
&\leq& \frac{B}{m} \sqrt{\EE \sum_{i=1}^m \| x_i \|^2} \leq \frac{BR}{\sqrt{m}}.
\end{eqnarray*}

Therefore, $\Rad_S(\Fcal) \leq L \Rad_S(\Gcal) \leq LBR \cdot \sqrt{\frac1m}$. Plugging into Equation~\eqref{eqn:uc-rad}, and using the elementary inequality that $\sqrt{C}+\sqrt{D} \leq \sqrt{2(C+D)}$, we have that with probability $1-\delta'$,
\begin{equation}
\EE_{(x,y) \sim D} \phi(y \inner{w}{x})
\leq \EE_{(x,y) \sim S} \phi(y \inner{w}{x})
+ L B R \cdot \sqrt{\frac{8 + 4\ln(\frac{2}{\delta'})}{m}}.
\label{eqn:uc-lip}
\end{equation}

%Applying the lemma, we immediately have
%\[ \Rad_S(\Fcal) \leq L \Rad_S(\Gcal) = L \frac{1}{m} \EE_{\sigma_1,\ldots,\sigma_m \sim \R} \sup_{w: \|w\|_2 \leq B} \sum_{i=1}^m \sigma_i y_i \inner{w}{x_i}. \]


\paragraph{Proof of the contraction lemma.} Recall that
\[ m \Rad_S(\Fcal) = \EE \sup_{f \in \Fcal} \sum_{i=1}^m \phi(f(z_i)). \]
Our high-level strategy is that, consider removing one $\phi$ at one location at a time, that is, to show
\begin{eqnarray*}
  && \EE \sup_{f \in \Fcal} \del{\sum_{i=1}^m \sigma_i \phi(f(z_i))}\\
  &\leq& \EE \sup_{f \in \Fcal} \del{L \sigma_1 f(z_1) + \sum_{i=2}^m \sigma_i \phi(f(z_i))}\\
   &\leq& \EE \sup_{f \in \Fcal} \del{L \sigma_1 f(z_1) + L \sigma_2 f(z_2) + \sum_{i=3}^m \sigma_i \phi(f(z_i))} \\
  &\leq& \ldots \leq \EE \sup_{f \in \Fcal} \del{L \sigma_1 f(z_1) + L \sigma_2 f(z_2) + \ldots + L \sigma_m f(z_m)}.
\end{eqnarray*}

We only show the first inequality; the rest steps are fairly similar. We first  expand the left hand side by explicitly averaging over random choices of $\sigma_1$:
\[ \EE \sup_{f \in \Fcal} \del{\sum_{i=1}^m \sigma_i \phi(f(z_i))}
 =
 \EE \frac12 \sbr{ \sup_{f \in \Fcal} \del{ \phi(f(z_1)) + \sum_{i=2}^m\sigma_i \phi(f(z_i))} + \sup_{f' \in \Fcal}\del{ -\phi(f'(z_1)) + \sum_{i=2}^m\sigma_i \phi(f'(z_i))}  }
\]
Note that the right hand side can also be written as:
\begin{equation}
\EE \frac12 \sbr{ \sup_{f, f' \in \Fcal} \del{ \phi(f(z_1)) - \phi(f'(z_1)) + \sum_{i=2}^m\sigma_i \phi(f(z_i)) + \sum_{i=2}^m\sigma_i \phi(f'(z_i))}}
\label{eqn:double-sup}
\end{equation}
which can be bounded as follows:
\begin{eqnarray*}
\text{~\eqref{eqn:double-sup}} &\leq&
\EE \frac12 \sbr{ \sup_{f, f' \in \Fcal} \del{ L\abs{f(z_1) - f'(z_1)} + \sum_{i=2}^m\sigma_i \phi(f(z_i)) + \sum_{i=2}^m\sigma_i \phi(f'(z_i))}} \\
&\leq&
\EE \frac12 \sbr{ \sup_{f, f' \in \Fcal} \del{ Lf(z_1) - Lf'(z_1) + \sum_{i=2}^m\sigma_i \phi(f(z_i)) + \sum_{i=2}^m\sigma_i \phi(f'(z_i))}} \\
&=&
\EE \frac12 \sbr{ \sup_{f \in \Fcal} \del{ Lf(z_1) + \sum_{i=2}^m\sigma_i \phi(f(z_i))} + \sup_{f' \in \Fcal}\del{ -Lf'(z_1) + \sum_{i=2}^m\sigma_i \phi(f'(z_i))}  } \\
&=&
\EE \sup_{f \in \Fcal} \del{L \sigma_1 f(z_1) + \sum_{i=2}^m \sigma_i \phi(f(z_i))}.
\end{eqnarray*}
where the first inequality uses the Lipschitzness of $\phi$, the first equality is based on the observation that there is always a pair of $f$ and $f'$ such that $f(z_1) - f'(z_1) \geq 0$ that achieves the supremum\footnote{within arbitrary precision.} - if $f(z_1) - f'(z_1) < 0$, then switch the settings of $f$ and $f'$ will give the same objective value inside the parenthesis. The second inequality is by unpacking the double supremum to the sum of two suprema (roughly speaking, ``undoing'' the operation in~\label{eqn:double-sup}), and the third inequality is by introducing the Rademacher random variable $\sigma_1$ back.

%we can achieve the

%\EE \sup_{f \in \Fcal} \del{\sum_{i=1}^m \sigma_i \phi(f(z_i))}

%Generally, if we are given a function $K: \Xcal \times \Xcal \to \RR$, under what condition is it a kernel (i.e. there exists a feature map $\phi$ such that $K(x,x') = \inner{\phi(x)}{\phi(x')}$)? We state the following theorem.
%\begin{theorem}[Mercer]
%A function $K$ is a kernel
%\end{theorem}

%It can be verified by the basic properties above that $k(x,x')$ are kernel functions. Here is one feature map


%Can you find out their respective feature mappings?

%\begin{theorem}[Sion's minimax theorem]
%Suppose $f(x,y)$ is a convex-concave function, i.e. for any fixed $y_0$, $f(x,y_0)$ is a convex function in $x$, and for any fixed $x_0$, $f(x_0, y)$ is a concave function in $y$. In addition suppose
%\end{theorem}

%Suppose $(w^\star, b^\star, A^\star)$ is optimal for~\eqref{eqn:orig}. By linear separability, $A^\star > 0$, which implies that $y_i(\inner{w^\star}{x_i} + b^\star) = |\inner{w^\star}{x_i} + b^\star| \geq A^\star$. This implies that


 %The optimal solutions in both optimization problems must satisfy that $A > 0$, which implies that


%that is, find the hyperplane that maximize the minimum distance to all training examples

%if the supports of positive and negative examples are in two clusters far away from each other, then the classifier we find would
%


\bibliographystyle{plain}
\bibliography{learning}
%\section{}


\end{document}
