<!doctype html>
<html >
<head>

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="/templates/pandoc-bootstrap-adaptive-template/tufte.css" />

   <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />



<script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script type='text/javascript' src='/templates/pandoc-bootstrap-adaptive-template/menu/js/jquery.cookie.js'></script>
<script type='text/javascript' src='/templates/pandoc-bootstrap-adaptive-template/menu/js/jquery.hoverIntent.minified.js'></script>
<script type='text/javascript' src='/templates/pandoc-bootstrap-adaptive-template/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

<link href="/templates/pandoc-bootstrap-adaptive-template/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
<link href="/templates/pandoc-bootstrap-adaptive-template/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
<link href="/templates/pandoc-bootstrap-adaptive-template/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  <script src="/templates/pandoc-bootstrap-adaptive-template/script.js"></script>

    <script src="/bower_components/sticky-kit/jquery.sticky-kit.js "></script>
  <meta name="generator" content="pandoc" />
  <title>CSC 696H Fall 2021: Presentation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="bootstrap.css" />
</head>
<body>

<!--
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">CSC 696H Fall 2021: Presentation</span>
        <ul class="nav pull-right doc-info">
                            </ul>
      </div>
    </div>
  </div>
  -->
  <div class="container">
    <div class="row">
            <div class="span12">
                        <nav class="navbar navbar-default">
                    <div class="container-fluid">
                      <!-- Brand and toggle get grouped for better mobile display -->
                      <div class="navbar-header">
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                          <span class="sr-only">Toggle navigation</span>
                          <span class="icon-bar"></span>
                          <span class="icon-bar"></span>
                          <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand" href="index.html">CSC 696H</a>
                      </div>
      
                      <!-- Collect the nav links, forms, and other content for toggling -->
                      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                        <ul class="nav navbar-nav">
                          <li><a href="schedule.html">Schedule</a></li>
                          <!-- <li><a href="syllabus.html">Syllabus</a></li> -->
                          <li><a href="presentation.html">Presentation</a></li>
                        </ul>
                      </div><!-- /.navbar-collapse -->
                    </div><!-- /.container-fluid -->
                  </nav>
            <h2 id="presentation-information">Presentation information</h2>
<h3 id="general-information">General information</h3>
<p>All registered students will be asked to give a 60-min in-class presentation on a RL theory paper of their choice. The paper should either come from the provided list of papers, or upon instructor approval. Before their presentation, a student is required to schedule a meeting with the instructor to discuss their presentation materials (slides, lecture notes, etc). Throughout the course, students are highly encouraged to meet with the instructors regularly on paper choices, reading progress, etc.</p>
<p>Here are some useful guidelines for:</p>
<p><em>reading papers: <a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">How to read a paper</a> by Prof. Srinivasan Keshav; <a href="https://cs.stanford.edu/~rishig/courses/ref/paper-reading-technical.pdf">Reading in Algorithms: Paper-Reading Survival Kit</a> by Prof. Tim Roughgarden. </em>presentations: <a href="https://people.eecs.berkeley.edu/~jrs/speaking.html">Giving an Academic Talk</a> by Prof. Jonathan Shewchuk; <a href="https://cseweb.ucsd.edu//~elkan/254/speaking.html">Notes on Giving a Research Talk</a> by Prof. Charles Elkan.</p>
<h3 id="paper-presentation-ideas-under-construction">Paper presentation ideas (under construction)</h3>
<p>Below are a few example research topics in reinforcement learning theory, each with a few “seed papers”; you can use the related work section in these papers, or use the “cited by” functionality in e.g. <a href="https://scholar.google.com/">google scholar</a> to find more papers on the same topic. Please also refer to proceeding pages of recent machine learning / learning theory conferences and workshops for more presentation ideas, such as:</p>
<ul>
<li><a href="http://proceedings.mlr.press/v97/">ICML</a>,</li>
<li><a href="http://proceedings.mlr.press/v99/">COLT</a>,</li>
<li><a href="https://papers.nips.cc/">NeurIPS</a> ,</li>
<li><a href="http://proceedings.mlr.press/v108/">AISTATS</a>,</li>
<li><a href="http://proceedings.mlr.press/v117/">ALT</a>,</li>
<li><a href="https://wensun.github.io/rl_theory_workshop_2020_ICML.github.io/#papers">ICML 2020 workshop on theoretical foundations of RL</a></li>
<li><a href="https://lyang36.github.io/icml2021_rltheory/">ICML 2021 workshop on RL theory</a></li>
</ul>
<p>Courses / tutorials in the RL theory research community may also have interesting reference papers good for presentation, for example:</p>
<ul>
<li><a href="https://nanjiang.cs.illinois.edu/cs598project/">Statistical Reinforcement Learning</a> by Nan Jiang</li>
<li><a href="http://alekhagarwal.net/bandits_and_rl/#view3">Bandits and RL</a> by Alekh Agarwal and Alex Slivkins</li>
<li><a href="https://sites.google.com/view/rltheoryseminars/home">RL theory virtual seminars</a> by Gergely Neu, Ciara Pike-Burke, and Csaba Szepesvari</li>
<li><a href="https://rltheorybook.github.io/colt21tutorial">COLT 2021 Tutorial: Statistical Foundations of Reinforcement Learning</a> by Akshay Krishnamurthy and Wen Sun</li>
<li><a href="https://rlgammazero.github.io/">AAAI 2020 and ALT 2019 Tutorials: Exploration-Exploitation in Reinforcement Learning</a> by Ronan Fruit, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta</li>
<li><a href="https://hunch.net/~tforl/">FOCS 2020 Tutorial: Theoretical Foundations of Reinforcement Learning</a> by Alekh Agarwal, Akshay Krishnamurthy, and John Langford</li>
</ul>
<h4 id="pac-rl">PAC RL</h4>
<ul>
<li>Andrew Wagenmaker, Max Simchowitz, Kevin Jamieson. Beyond No Regret: Instance-Dependent PAC Reinforcement Learning. 2021.</li>
<li>Lihong Li, Michael L. Littman, Thomas J. Walsh, Alexander L. Strehl. Knows what it knows: a framework for self-aware learning. Machine Learning, 2011.</li>
<li>Nan Jiang. <a href="http://nanjiang.cs.illinois.edu/files/cs598/note7.pdf">Notes on Rmax exploration.</a></li>
</ul>
<h4 id="regret-minimization-in-rl">Regret minimization in RL</h4>
<ul>
<li>Thomas Jaksch, Ronald Ortner, Peter Auer. Near-optimal Regret Bounds for Reinforcement Learning. JMLR 2010.</li>
<li>Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement learning. ICML, 2017.</li>
<li>Christoph Dann, Tor Lattimore, Emma Brunskill. Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning. NeurIPS 2017.</li>
<li>Andrea Zanette, Emma Brunskill. Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds. ICML 2019.</li>
<li>Max Simchowitz, Kevin Jamieson. Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs. NeurIPS 2019.</li>
<li>Haike Xu, Tengyu Ma, Simon S. Du. Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap. COLT 2021.</li>
</ul>
<h4 id="q-learning">Q-learning</h4>
<ul>
<li>Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael I. Jordan. Is Q-learning Provably Efficient? NeurIPS 2018.</li>
<li>Zihan Zhang, Yuan Zhou, Xiangyang Ji, Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition. NeurIPS 2020.</li>
<li>Kunhe Yang, Lin F. Yang, Simon S. Du. Q-learning with Logarithmic Regret. AISTATS 2021.</li>
<li>Kefan Dong, Yuanhao Wang, Xiaoyu Chen, Liwei Wang. Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP. ICLR 2020.</li>
</ul>
<h4 id="function-approximation">Function approximation</h4>
<ul>
<li>Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement learning with linear function approximation. COLT 2020.</li>
<li>Lin F. Yang and Mengdi Wang. Reinforcement Learning in feature space: Matrix bandit, kernels, and regret bound. ICML 2020.</li>
<li>Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent Bellman error. ICML 2020.</li>
<li>Ruosong Wang, Ruslan Salakhutdinov, and Lin F. Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. NeurIPS 2020.</li>
<li>Aditya Modi, Nan Jiang, Ambuj Tewari, Satinder Singh. Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles. AISTATS 2020.</li>
<li>Dongruo Zhou, Quanquan Gu, Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. COLT 2021.</li>
<li>Dylan J. Foster, Alexander Rakhlin, David Simchi-Levi, Yunzong Xu. Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective.</li>
<li>Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, Michael I. Jordan. On Function Approximation in Reinforcement Learning: Optimism in the Face of Large State Spaces. NeurIPS 2020.</li>
</ul>
<h4 id="nonparametric-rl">Nonparametric RL</h4>
<ul>
<li>Tongyi Cao, Akshay Krishnamurthy. Provably adaptive reinforcement learning in metric spaces. NeurIPS 2020.</li>
<li>Sean R. Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, Christina Lee Yu. Adaptive Discretization for Model-Based Reinforcement Learning. NeurIPS 2020.</li>
<li>Ahmed Touati, Adrien Ali Taiga, Marc G. Bellemare. Zooming for Efficient Model-Free Reinforcement Learning in Metric Spaces. 2020.</li>
<li>Zhao Song, Wen Sun. Efficient Model-free Reinforcement Learning in Metric Spaces. 2019.</li>
<li>Devavrat Shah, Qiaomin Xie. Q-Learning with Nearest Neighbors. NeurIPS 2018.</li>
</ul>
<h4 id="model-selection-in-online-rl">Model selection in online RL</h4>
<ul>
<li>Jonathan N. Lee, Aldo Pacchiano, Vidya Muthukumar, Weihao Kong, Emma Brunskill. Online Model Selection for Reinforcement Learning with Function Approximation. AISTATS 2021.</li>
<li>Aldo Pacchiano, Christoph Dann, Claudio Gentile, Peter Bartlett. Regret Bound Balancing and Elimination for Model Selection in Bandits and RL. 2020.</li>
<li>Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, Manish Purohit. Dynamic Balancing for Model Selection in Bandits and RL. ICML 2021.</li>
</ul>
<h4 id="multi-task-rl">Multi-task RL</h4>
<ul>
<li>Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, Liwei Wang. Near-optimal Representation Learning for Linear Bandits and Linear RL. ICML 2021.</li>
<li>Rui Lu, Gao Huang, Simon S. Du. On the Power of Multitask Representation Learning in Linear MDP. 2021.</li>
</ul>
<h4 id="rl-with-constraints">RL with constraints</h4>
<ul>
<li>Sobhan Miryoosefi, Kianté Brantley, Hal Daumé III, Miroslav Dudik, Robert Schapire. Reinforcement Learning with Convex Constraints. NeurIPS 2019.</li>
<li>Kianté Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, Wen Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. NeurIPS 2020.</li>
<li>Xiaoyu Chen, Jiachen Hu, Lihong Li, Liwei Wang. Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL. ICLR 2021.</li>
</ul>
<h4 id="corruption-robust-rl">Corruption-robust RL</h4>
<ul>
<li>Thodoris Lykouris, Max Simchowitz, Alex Slivkins, Wen Sun. Corruption-robust exploration in episodic reinforcement learning. COLT 2021.</li>
<li>Yifang Chen, Simon S. Du, Kevin Jamieson. Improved Corruption Robust Algorithms for Episodic Reinforcement Learning. ICML 2021.</li>
<li>Tianhao Wu, Yunchang Yang, Simon Du, Liwei Wang. On Reinforcement Learning with Adversarial Corruption and Its Application to Block MDP. ICML 2021.</li>
<li>Xuezhou Zhang, Yiding Chen, Jerry Zhu, Wen Sun. Corruption-Robust Offline Reinforcement Learning. 2021.</li>
<li>Tiancheng Jin, Longbo Huang, and Haipeng Luo. The Best of Both Worlds: Stochastic and Adversarial Episodic MDPs with Unknown Transition. 2021.</li>
</ul>
<h4 id="reward-free-exploration">Reward-free exploration</h4>
<ul>
<li>Chi Jin, Akshay Krishnamurthy, Max Simchowitz, Tiancheng Yu. Reward-Free Exploration for Reinforcement Learning. ICML 2020.</li>
<li>Emilie Kaufmann, Pierre Ménard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, Michal Valko. Adaptive Reward-Free Exploration. ALT 2021.</li>
<li>Ruosong Wang, Simon S. Du, Lin F. Yang, Ruslan Salakhutdinov. On Reward-Free Reinforcement Learning with Linear Function Approximation. NeurIPS 2020.</li>
</ul>
<h4 id="rl-in-adversarial-mdps">RL in adversarial MDPs</h4>
<ul>
<li>Tiancheng Jin and Haipeng Luo. Simultaneously Learning Stochastic and Adversarial Episodic MDPs with Known Transition. NeurIPS 2020.</li>
<li>Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning Adversarial Markov Decision Processes with Bandit Feedback and Unknown Transition.</li>
</ul>
<h4 id="imitation-learning">Imitation learning</h4>
<h5 id="interactive-imitation-learning">Interactive Imitation Learning</h5>
<ul>
<li>Hal Daumé III, John Langford, Daniel Marcu. Search-based Structured Prediction. Machine Learning Journal 2009.</li>
<li>Stephane Ross, Drew Bagnell. Efficient Reductions for Imitation Learning. AISTATS 2010.</li>
<li>Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell. A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. AISTATS 2011.</li>
<li>Stephane Ross, J. Andrew Bagnell. Reinforcement and Imitation Learning via Interactive No-Regret Learning. NeurIPS 2014.</li>
<li>Wen Sun, Arun Venkatraman, Geoffrey J. Gordon, Byron Boots, J. Andrew Bagnell. Deeply AggreVaTeD: differentiable imitation learning for sequential prediction. ICML 2017.</li>
<li>Ching-An Cheng and Byron Boots. Convergence of Value Aggregation for Imitation Learning. AISTATS 2018.</li>
<li>Wen Sun, Anirudh Vemula, Byron Boots, J. Andrew Bagnell. Provably Efficient Imitation Learning from Observation Alone. ICML 2019.</li>
</ul>
<h5 id="apprenticeship-learning-and-inverse-reinforcement-learning">Apprenticeship Learning and Inverse Reinforcement Learning</h5>
<ul>
<li>Pieter Abbeel and Andrew Y. Ng. Apprenticeship Learning via Inverse Reinforcement Learning. ICML 2004.</li>
<li>Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, Anind K. Dey. Maximum Entropy Inverse Reinforcement Learning. AAAI 2008.</li>
<li>Brian D. Ziebart, J.Andrew Bagnell, Anind K. Dey. Modeling Interaction via the Principle of Maximum Causal Entropy. ICML 2010.</li>
<li>Umar Syed and Robert E. Schapire. A Game-Theoretic Approach to Apprenticeship Learning. NeurIPS 2007.</li>
<li>Alekh Agarwal, Ashwinkumar Badanidiyuru, Miroslav Dudik, Robert Schapire, Aleksandrs Slivkins, Miro Dudík. Robust Multi-objective Learning with Mentor Feedback. COLT 2014.</li>
<li>Jonathan Ho, Stefano Ermon. Generative Adversarial Imitation Learning. NeurIPS 2016.</li>
<li>Kareem Amin, Nan Jiang, Satinder Singh. Repeated Inverse Reinforcement Learning. NeurIPS 2017.</li>
</ul>
            </div>
    </div>
  </div>


  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
