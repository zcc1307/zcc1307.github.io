<!doctype html>
<html >
<head>

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />

  <link rel="stylesheet" type="text/css" href="/templates/pandoc-bootstrap-adaptive-template/tufte.css" />

   <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />



<script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script type='text/javascript' src='/templates/pandoc-bootstrap-adaptive-template/menu/js/jquery.cookie.js'></script>
<script type='text/javascript' src='/templates/pandoc-bootstrap-adaptive-template/menu/js/jquery.hoverIntent.minified.js'></script>
<script type='text/javascript' src='/templates/pandoc-bootstrap-adaptive-template/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

<link href="/templates/pandoc-bootstrap-adaptive-template/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
<link href="/templates/pandoc-bootstrap-adaptive-template/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
<link href="/templates/pandoc-bootstrap-adaptive-template/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  <script src="/templates/pandoc-bootstrap-adaptive-template/script.js"></script>

    <script src="/bower_components/sticky-kit/jquery.sticky-kit.js "></script>
  <meta name="generator" content="pandoc" />
  <title>CSC 696H Fall 2021: Presentation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="bootstrap.css" />
</head>
<body>

<!--
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">CSC 696H Fall 2021: Presentation</span>
        <ul class="nav pull-right doc-info">
                            </ul>
      </div>
    </div>
  </div>
  -->
  <div class="container">
    <div class="row">
            <div class="span12">
                        <nav class="navbar navbar-default">
                    <div class="container-fluid">
                      <!-- Brand and toggle get grouped for better mobile display -->
                      <div class="navbar-header">
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                          <span class="sr-only">Toggle navigation</span>
                          <span class="icon-bar"></span>
                          <span class="icon-bar"></span>
                          <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand" href="index.html">CSC 696H</a>
                      </div>
      
                      <!-- Collect the nav links, forms, and other content for toggling -->
                      <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                        <ul class="nav navbar-nav">
                          <li><a href="schedule.html">Schedule</a></li>
                          <!-- <li><a href="syllabus.html">Syllabus</a></li> -->
                          <li><a href="presentation.html">Presentation</a></li>
                        </ul>
                      </div><!-- /.navbar-collapse -->
                    </div><!-- /.container-fluid -->
                  </nav>
            <h2 id="presentation-information">Presentation information</h2>
<h3 id="general-information">General information</h3>
<p>All registered students will be asked to give a 60-min in-class presentation on a RL theory paper of their choice. The paper should either come from the provided list of papers, or upon instructor approval. Before their presentation, a student is required to schedule a meeting with the instructor to discuss their presentation materials (slides, lecture notes, etc). Throughout the course, students are highly encouraged to meet with the instructors regularly on paper choices, reading progress, etc.</p>
<p>Here are some useful guidelines for:</p>
<ul>
<li>reading papers: <a href="http://ccr.sigcomm.org/online/files/p83-keshavA.pdf">How to read a paper</a> by Prof. Srinivasan Keshav; <a href="https://cs.stanford.edu/~rishig/courses/ref/paper-reading-technical.pdf">Reading in Algorithms: Paper-Reading Survival Kit</a> by Prof. Tim Roughgarden.</li>
<li>presentations: <a href="https://people.eecs.berkeley.edu/~jrs/speaking.html">Giving an Academic Talk</a> by Prof. Jonathan Shewchuk; <a href="https://cseweb.ucsd.edu//~elkan/254/speaking.html">Notes on Giving a Research Talk</a> by Prof. Charles Elkan.</li>
</ul>
<h3 id="paper-presentation-ideas-under-construction">Paper presentation ideas (under construction)</h3>
<p>Below are a few example research topics in reinforcement learning theory, each with a few “seed papers”; you can use the related work section in these papers, or use the “cited by” functionality in e.g. <a href="https://scholar.google.com/">google scholar</a> to find more papers on the same topic. Please also refer to proceeding pages of recent machine learning / learning theory conferences and workshops for more presentation ideas, such as:</p>
<ul>
<li><a href="http://proceedings.mlr.press/v97/">ICML</a>,</li>
<li><a href="http://proceedings.mlr.press/v99/">COLT</a>,</li>
<li><a href="https://papers.nips.cc/">NeurIPS</a> ,</li>
<li><a href="http://proceedings.mlr.press/v108/">AISTATS</a>,</li>
<li><a href="http://proceedings.mlr.press/v117/">ALT</a>,</li>
<li><a href="https://wensun.github.io/rl_theory_workshop_2020_ICML.github.io/#papers">ICML 2020 workshop on theoretical foundations of RL</a></li>
<li><a href="https://lyang36.github.io/icml2021_rltheory/">ICML 2021 workshop on RL theory</a></li>
</ul>
<p>Courses / tutorials in the RL theory research community may also have interesting reference papers good for presentation, for example:</p>
<ul>
<li><a href="https://nanjiang.cs.illinois.edu/cs598project/">Statistical Reinforcement Learning</a> by Nan Jiang</li>
<li><a href="http://alekhagarwal.net/bandits_and_rl/#view3">Bandits and RL</a> by Alekh Agarwal and Alex Slivkins</li>
<li><a href="https://sites.google.com/view/rltheoryseminars/home">RL theory virtual seminars</a> by Gergely Neu, Ciara Pike-Burke, and Csaba Szepesvari</li>
<li><a href="https://rltheorybook.github.io/colt21tutorial">COLT 2021 Tutorial: Statistical Foundations of Reinforcement Learning</a> by Akshay Krishnamurthy and Wen Sun</li>
<li><a href="https://rlgammazero.github.io/">AAAI 2020 and ALT 2019 Tutorials: Exploration-Exploitation in Reinforcement Learning</a> by Ronan Fruit, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta</li>
<li><a href="https://hunch.net/~tforl/">FOCS 2020 Tutorial: Theoretical Foundations of Reinforcement Learning</a> by Alekh Agarwal, Akshay Krishnamurthy, and John Langford</li>
</ul>
<h4 id="rl-with-a-generative-model">RL with a generative model</h4>
<ul>
<li>Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. Machine learning, 2013</li>
<li>Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen. Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model. NeurIPS 2020.</li>
<li>Aaron Sidford, Mengdi Wang, Xian Wu, Lin F. Yang, and Yinyu Ye. Near-optimal time and sample complexities for solving discounted markov decision process with a generative model. NeurIPS 2018.</li>
<li>Alekh Agarwal, Sham Kakade, and Lin F. Yang. Model-based reinforcement learning with a generative model is minimax optimal. COLT 2020.</li>
</ul>
<h4 id="pac-rl">PAC RL</h4>
<ul>
<li>Andrew Wagenmaker, Max Simchowitz, Kevin Jamieson. Beyond No Regret: Instance-Dependent PAC Reinforcement Learning. 2021.</li>
<li>Lihong Li, Michael L. Littman, Thomas J. Walsh, Alexander L. Strehl. Knows what it knows: a framework for self-aware learning. Machine Learning, 2011.</li>
<li>Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, Michal Valko. Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited. ALT 2021.</li>
</ul>
<h4 id="regret-minimization-in-rl">Regret minimization in RL</h4>
<ul>
<li>Thomas Jaksch, Ronald Ortner, Peter Auer. Near-optimal Regret Bounds for Reinforcement Learning. JMLR 2010.</li>
<li>Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement learning. ICML, 2017.</li>
<li>Christoph Dann, Tor Lattimore, Emma Brunskill. Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning. NeurIPS 2017.</li>
<li>Andrea Zanette, Emma Brunskill. Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds. ICML 2019.</li>
<li>Max Simchowitz, Kevin Jamieson. Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs. NeurIPS 2019.</li>
<li>Haike Xu, Tengyu Ma, Simon S. Du. Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap. COLT 2021.</li>
</ul>
<h4 id="q-learning">Q-learning</h4>
<ul>
<li>Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael I. Jordan. Is Q-learning Provably Efficient? NeurIPS 2018.</li>
<li>Zihan Zhang, Yuan Zhou, Xiangyang Ji, Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition. NeurIPS 2020.</li>
<li>Kunhe Yang, Lin F. Yang, Simon S. Du. Q-learning with Logarithmic Regret. AISTATS 2021.</li>
<li>Kefan Dong, Yuanhao Wang, Xiaoyu Chen, Liwei Wang. Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP. ICLR 2020.</li>
</ul>
<h4 id="function-approximation">Function approximation</h4>
<ul>
<li>Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement learning with linear function approximation. COLT 2020.</li>
<li>Lin F. Yang and Mengdi Wang. Reinforcement Learning in feature space: Matrix bandit, kernels, and regret bound. ICML 2020.</li>
<li>Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent Bellman error. ICML 2020.</li>
<li>Ruosong Wang, Ruslan Salakhutdinov, and Lin F. Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. NeurIPS 2020.</li>
<li>Aditya Modi, Nan Jiang, Ambuj Tewari, Satinder Singh. Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles. AISTATS 2020.</li>
<li>Dongruo Zhou, Quanquan Gu, Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. COLT 2021.</li>
<li>Dylan J. Foster, Alexander Rakhlin, David Simchi-Levi, Yunzong Xu. Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective.</li>
<li>Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, Michael I. Jordan. On Function Approximation in Reinforcement Learning: Optimism in the Face of Large State Spaces. NeurIPS 2020.</li>
</ul>
<h4 id="nonparametric-rl">Nonparametric RL</h4>
<ul>
<li>Tongyi Cao, Akshay Krishnamurthy. Provably adaptive reinforcement learning in metric spaces. NeurIPS 2020.</li>
<li>Sean R. Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, Christina Lee Yu. Adaptive Discretization for Model-Based Reinforcement Learning. NeurIPS 2020.</li>
<li>Ahmed Touati, Adrien Ali Taiga, Marc G. Bellemare. Zooming for Efficient Model-Free Reinforcement Learning in Metric Spaces. 2020.</li>
<li>Zhao Song, Wen Sun. Efficient Model-free Reinforcement Learning in Metric Spaces. 2019.</li>
<li>Devavrat Shah, Qiaomin Xie. Q-Learning with Nearest Neighbors. NeurIPS 2018.</li>
</ul>
<h4 id="model-selection-in-online-rl">Model selection in online RL</h4>
<ul>
<li>Jonathan N. Lee, Aldo Pacchiano, Vidya Muthukumar, Weihao Kong, Emma Brunskill. Online Model Selection for Reinforcement Learning with Function Approximation. AISTATS 2021.</li>
<li>Aldo Pacchiano, Christoph Dann, Claudio Gentile, Peter Bartlett. Regret Bound Balancing and Elimination for Model Selection in Bandits and RL. 2020.</li>
<li>Ashok Cutkosky, Christoph Dann, Abhimanyu Das, Claudio Gentile, Aldo Pacchiano, Manish Purohit. Dynamic Balancing for Model Selection in Bandits and RL. ICML 2021.</li>
</ul>
<h4 id="multi-task-rl">Multi-task RL</h4>
<ul>
<li>Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, Liwei Wang. Near-optimal Representation Learning for Linear Bandits and Linear RL. ICML 2021.</li>
<li>Rui Lu, Gao Huang, Simon S. Du. On the Power of Multitask Representation Learning in Linear MDP. 2021.</li>
</ul>
<h4 id="rl-with-constraints">RL with constraints</h4>
<ul>
<li>Sobhan Miryoosefi, Kianté Brantley, Hal Daumé III, Miroslav Dudik, Robert Schapire. Reinforcement Learning with Convex Constraints. NeurIPS 2019.</li>
<li>Kianté Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, Wen Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. NeurIPS 2020.</li>
<li>Xiaoyu Chen, Jiachen Hu, Lihong Li, Liwei Wang. Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL. ICLR 2021.</li>
</ul>
<h4 id="corruption-robust-rl">Corruption-robust RL</h4>
<ul>
<li>Thodoris Lykouris, Max Simchowitz, Alex Slivkins, Wen Sun. Corruption-robust exploration in episodic reinforcement learning. COLT 2021.</li>
<li>Yifang Chen, Simon S. Du, Kevin Jamieson. Improved Corruption Robust Algorithms for Episodic Reinforcement Learning. ICML 2021.</li>
<li>Tianhao Wu, Yunchang Yang, Simon Du, Liwei Wang. On Reinforcement Learning with Adversarial Corruption and Its Application to Block MDP. ICML 2021.</li>
<li>Xuezhou Zhang, Yiding Chen, Jerry Zhu, Wen Sun. Corruption-Robust Offline Reinforcement Learning. 2021.</li>
<li>Tiancheng Jin, Longbo Huang, and Haipeng Luo. The Best of Both Worlds: Stochastic and Adversarial Episodic MDPs with Unknown Transition. 2021.</li>
</ul>
<h4 id="reward-free-exploration-active-learning">Reward-free exploration; Active learning</h4>
<ul>
<li>Chi Jin, Akshay Krishnamurthy, Max Simchowitz, Tiancheng Yu. Reward-Free Exploration for Reinforcement Learning. ICML 2020.</li>
<li>Emilie Kaufmann, Pierre Ménard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, Michal Valko. Adaptive Reward-Free Exploration. ALT 2021.</li>
<li>Ruosong Wang, Simon S. Du, Lin F. Yang, Ruslan Salakhutdinov. On Reward-Free Reinforcement Learning with Linear Function Approximation. NeurIPS 2020.</li>
<li>Pierre Ménard, Omar Darwiche Domingues, Emilie Kaufmann, Anders Jonsson, Edouard Leurent, Michal Valko. Fast active learning for pure exploration in reinforcement learning. ICML 2021.</li>
</ul>
<h4 id="rl-in-adversarial-mdps">RL in adversarial MDPs</h4>
<ul>
<li>Eyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online Markov decision processes. Mathematics of Operations Research, 2009.</li>
<li>Zimin, A. and Neu, G. Online learning in episodic markovian decision processes by relative entropy policy search. NeurIPS 2013.</li>
<li>Rosenberg, A. and Mansour, Y. Online convex optimization in adversarial markov decision processes. ICML 2019.</li>
<li>Tiancheng Jin and Haipeng Luo. Simultaneously Learning Stochastic and Adversarial Episodic MDPs with Known Transition. NeurIPS 2020.</li>
<li>Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning Adversarial Markov Decision Processes with Bandit Feedback and Unknown Transition.</li>
</ul>
<h4 id="stochastic-games">Stochastic games</h4>
<ul>
<li>Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu. Online Reinforcement Learning in Stochastic Games. NeurIPS 2017.</li>
<li>Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. NeurIPS 2020.</li>
<li>Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning Zero-Sum Simultaneous Move Markov Games Using Function Approximation and Correlated Equilibrium. COLT 2020.</li>
<li>Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. ICML 2021.</li>
<li>Yi Tian, Yuanhao Wang, Tiancheng Yu, Suvrit Sra. Online Learning in Unknown Markov Games. ICML 2021.</li>
</ul>
<h4 id="rl-and-control">RL and control</h4>
<ul>
<li>Naman Agarwal, Brian Bullins, Elad Hazan, Sham M Kakade, and Karan Singh. Online control with adversarial disturbances. ICML 2019.</li>
<li>Dylan J Foster and Max Simchowitz. Logarithmic Regret for Adversarial Online Control. ICML 2020.</li>
<li>Naman Agarwal, Elad Hazan, Karan Singh. Logarithmic regret for online control. NeurIPS 2019.</li>
<li>Gautam Goel and Babak Hassibi. The power of linear controllers in LQR control. 2020.</li>
<li>Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear quadratic systems. COLT 2011.</li>
<li>Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators efficiently with only sqrt(T) regret. ICML 2019.</li>
<li>Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. NeurIPS 2018.</li>
</ul>
<h4 id="policy-optimization">Policy optimization</h4>
<ul>
<li>Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. 2020.</li>
<li>Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods, 2020.</li>
<li>Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. POLITEX: Regret bounds for policy iteration using expert prediction. ICML 2019.</li>
<li>Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization attains globally optimal policy. 2019.</li>
<li>Qi Cai, Zhuoran Yang, Chi Jin, Zhaoran Wang. Provably efficient exploration in policy optimization. ICML 2020.</li>
</ul>
<h4 id="rl-in-rich-observation-mdps">RL in rich-observation MDPs</h4>
<ul>
<li>Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire. On Oracle-Efficient PAC RL with Rich Observations. NeurIPS 2018.</li>
<li>Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, John Langford. Provably efficient RL with Rich Observations via Latent State Decoding. ICML 2019.</li>
<li>Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford. Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches. COLT 2019.</li>
<li>Kefan Dong, Jian Peng, Yining Wang, Yuan Zhou. -Regret for Learning in Markov Decision Processes with Function Approximation and Low Bellman Rank. COLT 2020.</li>
</ul>
<h4 id="imitation-learning-and-inverse-reinforcement-learning">Imitation learning and Inverse Reinforcement Learning</h4>
<ul>
<li>Wen Sun, Arun Venkatraman, Geoffrey J. Gordon, Byron Boots, J. Andrew Bagnell. Deeply AggreVaTeD: differentiable imitation learning for sequential prediction. ICML 2017.</li>
<li>Ching-An Cheng and Byron Boots. Convergence of Value Aggregation for Imitation Learning. AISTATS 2018.</li>
<li>Wen Sun, Anirudh Vemula, Byron Boots, J. Andrew Bagnell. Provably Efficient Imitation Learning from Observation Alone. ICML 2019.</li>
<li>Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, Steven Wu. Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap. ICML 2021.</li>
<li>Umar Syed and Robert E. Schapire. A Game-Theoretic Approach to Apprenticeship Learning. NeurIPS 2007.</li>
<li>Alekh Agarwal, Ashwinkumar Badanidiyuru, Miroslav Dudik, Robert Schapire, Aleksandrs Slivkins. Robust Multi-objective Learning with Mentor Feedback. COLT 2014.</li>
<li>Kareem Amin, Nan Jiang, Satinder Singh. Repeated Inverse Reinforcement Learning. NeurIPS 2017.</li>
<li>Nived Rajaraman, Lin F. Yang, Jiantao Jiao, Kannan Ramachandran. Toward the Fundamental Limits of Imitation Learning. NeurIPS 2020.</li>
<li>Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, Stuart Russell. Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. 2021.</li>
</ul>
            </div>
    </div>
  </div>


  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
