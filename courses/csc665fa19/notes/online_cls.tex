\documentclass{article}
\usepackage{fullpage}
\usepackage{physics}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{commath}
\usepackage{algorithm, algorithmic}
\usepackage{natbib}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}

\DeclareMathOperator*{\hinge}{{\rm hinge}}
\DeclareMathOperator*{\kl}{{\rm kl}}
\DeclareMathOperator*{\h}{{\rm h}}
\DeclareMathOperator*{\KL}{{\rm KL}}
\DeclareMathOperator*{\nent}{{\rm H}}
\DeclareMathOperator*{\Bin}{{\rm B}}
\DeclareMathOperator{\Rad}{{\mathrm{Rad}}}
\DeclareMathOperator*{\R}{{\rm R}}
\DeclareMathOperator*{\U}{{\rm U}}
\DeclareMathOperator*{\N}{{\rm N}}
\DeclareMathOperator*{\Var}{{\rm Var}}
\DeclareMathOperator*{\err}{{\rm err}}
\DeclareMathOperator*{\sign}{{\rm sign}}
\DeclareMathOperator*{\Xcal}{{\cal X}}
%\DeclareMathOperator*{\Hcal}{{\cal H}}
\DeclareMathOperator*{\Ycal}{{\cal Y}}
\DeclareMathOperator{\Acal}{{\cal A}}
\DeclareMathOperator*{\Bcal}{{\cal B}}
\DeclareMathOperator*{\Zcal}{{\cal Z}}
\DeclareMathOperator*{\Gcal}{{\cal G}}
\DeclareMathOperator*{\CH}{{\rm CH}}
\DeclareMathOperator{\Mcal}{{\cal M}}
%\DeclareMathOperator*{\Fcal}{{\cal F}}
\DeclareMathOperator*{\Scal}{{\cal S}}
\DeclareMathOperator*{\Ical}{{\cal I}}
\DeclareMathOperator*{\argmin}{{\rm argmin}}
\DeclareMathOperator*{\argmax}{{\rm argmax}}
\DeclareMathOperator*{\maximize}{{\rm maximize}}
\DeclareMathOperator*{\minimize}{{\rm minimize}}
\DeclareMathOperator*{\st}{{\rm s.t.}}
\DeclareMathOperator*{\VC}{{\rm VC}}
\DeclareMathOperator{\EE}{{\mathbb E}}
\DeclareMathOperator{\PP}{{\mathbb P}}
\newcommand{\RR}{\mathbb{R}} % Real numbers
%\newcommand{\EE}{\mathbb{E}}
%\newcommand{\PP}{\mathbb{P}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\defeq}{\triangleq}
\newcommand*{\one}{{\bf 1}}
\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}
\DeclareMathOperator*{\Ber}{{\rm Bernoulli}}

\title{CSC 665: Online classification}
\author{Chicheng Zhang}

\begin{document}
\maketitle

\section{Online learning}
\begin{enumerate}
\item Sequential decision making problem
\item Different from statistical learning, here training and test are interleaved
\item Has many applications, e.g. spam filtering, (personalized) product recommendation, experimental design, sequential investment, etc
\end{enumerate}

General setup:

\begin{algorithm}
  \caption{Online learning: general setup}
\begin{algorithmic}
\REQUIRE{Context space $\Xcal$, action space $\Acal$.}
\FOR{timesteps $t = 1,2,\ldots,T$:}
\STATE (Optional) Observe context $x_t \in \Xcal$
\STATE Take action $a_t \in \Acal$
\STATE Receive feedback $b_t$ (that reveals information about loss $\ell_t$)
\ENDFOR
\STATE Goal: minimize cumulative loss $\sum_{t=1}^T \ell_t(a_t)$
\end{algorithmic}
\end{algorithm}

Examples:
\begin{enumerate}
\item Spam filtering (online classification).
\begin{enumerate}
  \item Each $x_t$ (in $\Xcal = \RR^d$) denotes the feature representation
of an email.
  \item Use $\Acal = \cbr{\pm 1}$: $+1$ denotes non-spam, $-1$ denotes spam.
  \item Feedback $b_t = y_t$: true label of email
  \item Loss: $\ell_t(a) = \one(a \neq y_t)$ - classification error
\end{enumerate}


\item Spam filtering, modified (partial information online classification).
Same as the setup before, except that the feedback model is different:
\[ b_t = \begin{cases} y_t & a_t = -1 \\ \bot & a_t = +1 \\ \end{cases} \]
In words, if an email is classified as non-spam, then it goes to the inbox and user marks
spam if necessary; however if an email is classified as spam, then the user does not
check the spam folder and never provided feedback on it.

\item Product recommendation (multi-armed bandits).
\begin{enumerate}
  \item No context
  \item $\Acal = \cbr{1,\ldots,K}$: $K$ products
  \item Loss: $\ell_t(a)$ - the cost of recommending product $a$ to user $t$ (characterizing user's preferences on all products)
  \item Feedback: $b_t = \ell_t(a_t)$ - user's preferences on product recommended (but not other $K-1$ products)
\end{enumerate}

\item Personalized product recommendation (contextual bandits).
Same as the setup before, except that a context $x_t$ is given at each timestep $t$, that reveals ``charateristics'' about user $t$. The goal is to utilize the contexts to make better product recommendations.
\end{enumerate}

Some terminlogies:
\begin{enumerate}
  \item Full information vs. Partial information: if $b_t$ reveals the true loss function $\ell_t$, then it is called full-information setting; otherwise it is called partial-information setting. Both settings have many applications in practice.
  \item Stochastic vs. Adversarial: if $(x_t, \ell_t)$'s are iid, then it is called the stochastic setting (where techniques in statistical learning can potentially carry over); Adversarial setting refers to the setup where we don't have assumptions on the data generation process.
\end{enumerate}

\section{Online classification}
Convention: as $\Acal = \Ycal = \cbr{\pm 1}$, we often write $a_t$ as $\hat{y}_t$. The goal is to minimize the cumulative number of classification errors, $\sum_{t=1}^T \one(\hat{y}_t \neq y_t)$.

As a starting point, let's consider a simple setting when $\Hcal$ is finite, and we are in the realizable setting: there exists a classifier $h^\star$ in $\Hcal$ that agrees with all the examples. How can we design a learning algorithm that makes a small number of mistakes?

\paragraph{The consistency algorithm: a first trial.} One plausible idea is to utilize the consitency algorithm (or ERM algorithm) we studied in statistical learning: at time $t$, define version space $V_t = \cbr{h \in \Hcal: h(x_s) = y_s \forall s \leq t-1}$. The name {\em version space} comes from the fact that this set of classifiers gets constantly updated, therefore having ``versions'' indexed by timesteps.
The algorithm then selects a classifier $h_t \in V_t$ and uses that to make prediction: $\hat{y}_t = h_t(x_t)$.

\begin{theorem}
The consistency algorithm makes $|\Hcal| - 1$ mistakes (regardless of the length of time horizon $T$).
\label{thm:cons}
\end{theorem}

\begin{proof}
First, by the definition of version space, and the realizability assumption, the algorithm maintains the invariant that $h^\star \in V_t$.

Second, let $M_t = \one(\hat{y}_t \neq y_t)$  be the mistake indicator at time $t$. We claim that $M_t \leq |V_t| - |V_{t+1}|$. We look at two cases:
\begin{enumerate}
  \item if $M_t = 0$, as $V_{t+1} \subset V_{t}$, the right hand side is positive, thus the inequality is true.
  \item if $M_t = 1$, note that $h_t$ will be excluded from $V_t$. This implies that $|V_{t+1}| - |V_t| \geq 1$.
\end{enumerate}

Define potential $\Phi_t = |V_t|$; in this notation, $M_t \leq \Phi_t - \Phi_{t+1}$. Summing over all round $t$ from $1$ to $T$, we have
\[ \sum_{t=1}^T \leq \Phi_1 - \Phi_{T+1} \leq \Phi_1 = |\Hcal|. \]
\end{proof}

Can we design a better algorithm for online binary classification under realizability?
It turns out that using a more carefully designed prediction strategy we can do much better, reducing the mistake bound from linear in $|\Hcal|$ to only logarithmic.

\paragraph{The halving algorithm.} Same as the consistency algorithm, Halving will also keep track of version space, defined in the same way. At timestep $t$, observe that example $x_t$ divides the version space $V_t$ into two parts: $V_t^+ = \cbr{h \in V_t: h(x_t) = +1}$, and $V_t^- = \cbr{h \in V_t: h(x_t) = -1}$. We know that at the end of round $t$, one of them would become the update version space $V_{t+1}$.

The halving algorithm makes prediction as follows:
\[ \hat{y}_t = \begin{cases} +1 & |V_t^+| \geq |V_t| / 2, \\ -1 & \text{otherwise}, \end{cases} \]
which is equivalent to a majority vote over classifiers in $V_t$ (tie broken in favor of $+1$).

What is the advantage of this new prediction strategy? We claim that $M_t \leq \log |V_t| - \log |V_{t+1}|$. Why? We also conduct a case analysis on $M_t$:
\begin{enumerate}
  \item If $M_t = 0$, the right hand side is $\log |V_t| - \log |V_{t+1}|$, which is at least $0$; therefore the inequality is true.
  \item If $M_t = 1$, then $y_t = -\hat{y}_t$. It can be seen that from the definition of $\hat{y}_t$, $V_{t+1} = V_t^{y_t} = V_t^{-\hat{y}_t}$ is the minority class, implying that $|V_{t+1}| \leq |V_t| / 2$. The claimed inequality is shown by taking logarithms on both sides.
\end{enumerate}

Define potential $\Phi_t = \log|V_t|$; in this notation, $M_t \leq \Phi_t - \Phi_{t+1}$. Summing over all round $t$ from $1$ to $T$, we have
\[ \sum_{t=1}^T \leq \Phi_1 - \Phi_{T+1} \leq \Phi_1 = \log|\Hcal|. \]

To summarize, we have the following theorem.
\begin{theorem}
The halving algorithm makes $\log |\Hcal|$ mistakes (regardless of the length of time horizon $T$).
\label{thm:halv}
\end{theorem}

\section{Mistake bound model and the minimax analysis of realizable online learning}

\begin{definition}
  \begin{enumerate}
  \item Algorithm $\Acal$ is said to acheive a mistake bound $B$ with hypothesis class $\Hcal$,
  if for any sequence of examples $S$ realizable with respect to $\Hcal$, $\Mcal_{\Acal}(S)$,
  the cumulative number of mistakes made by $\Acal$ on $S$ is at most $B$.
  \item $\Hcal$ is online learnable if there exists an algorithm $\Acal$ such that $\Acal$ achieves a finite mistake bound on $\Hcal$.
\end{enumerate}
\end{definition}

In light of Theorem~\ref{thm:cons} or Theorem~\ref{thm:halv}, we have the following simple fact.

\begin{corollary}
Any finite hypothesis class $\Hcal$ is online learnable.
\end{corollary}

What happens for infinte hypothesis classes? Can we develop similar capacity measure like VC dimension that measures the fundamental complexity of online learning? Imaging online classification as a game between the learner and the environment, where the learning algorithm makes online predictions at every timestep that tries to minimize the number of mistakes, while the environment shows examples sequentially to ``trick'' the learner to make as many mistakes, without violating the realizability assumption. We can formulate the design of online classification algorithms as the following optimization problem:
\[ \min_{\Acal} \max_{S: S \text{ realizable by } \Hcal} \Mcal_{\Acal}(S). \]

In other words, we would like to design an algorithm, $\Acal$, that has the smallest number of worst case mistakes (where every sequence of example realizable by $\Hcal$ is a ``case'').

To analyze this, let us first look at the flip side: can we design a strategy of the environment to enforce any learner to make a lot of mistakes? To study this, we need the concept called mistake trees. (See handwritten notes for details.)

%What are some intrinsic measure of complexity of a hypothesis class that makes it hard to learn online?

\end{document}
